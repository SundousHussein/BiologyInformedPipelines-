{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:40:33.213980225Z",
     "start_time": "2024-02-09T05:40:31.459439571Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import pyreadstat\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:40:34.102473987Z",
     "start_time": "2024-02-09T05:40:34.100810695Z"
    }
   },
   "id": "113796de11d9f5c9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[27, 7], edge_index=[2, 78], edge_attr=[78], y=[27])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the Graph Dataset\n",
    "DATASET_DIR = '/home/shussein/NetCO/GNNs/data/COPD/SparsifiedNetworks'\n",
    "dataset_name = 'trimmed_fev1_0.515_0.111_adj.csv'\n",
    "\n",
    "graph_adj_file = os.path.join(DATASET_DIR, dataset_name)\n",
    "graph_adj = pd.read_csv(graph_adj_file, index_col=0).to_numpy()\n",
    "nodes_names = pd.read_csv(graph_adj_file, index_col=0).index.tolist()\n",
    "\n",
    "original_dataset = pd.read_csv(os.path.join(DATASET_DIR, 'fev1_X.csv'), index_col=0).reset_index(drop='index')\n",
    "dataset_associated_phenotype = pd.read_csv(os.path.join(DATASET_DIR, 'fev1_Y.csv'), index_col=0).reset_index(drop='index')\n",
    "\n",
    "# TODO: Attempting to run the GCN without predicting the phenotype; if this works -> remove this code\n",
    "nodes_correlation_with_phenotype = original_dataset.corrwith(dataset_associated_phenotype['FEV1pp_utah'])\n",
    "\n",
    "original_dataset_sid = pd.read_csv('../GNNs/data/COPD/SparsifiedNetworks/fev1_X.csv', index_col=0)\n",
    "original_dataset_sid.index.name = 'sid'\n",
    "\n",
    "clinical_variables, meta = pyreadstat.read_sas7bdat(\"../Data/COPDGene_P1P2P3_SM_NS_Long_Oct22.sas7bdat\")\n",
    "clinical_variables = clinical_variables.set_index('sid')\n",
    "# Filtering on Visit Number\n",
    "clinical_variables = clinical_variables[clinical_variables['visitnum'] == 2.0]\n",
    "\n",
    "clinical_variables_comorbidities = ['Angina', 'CongestHeartFail', 'CoronaryArtery', 'HeartAttack', 'PeriphVascular', 'Stroke', 'TIA', 'Diabetes',\n",
    "                                    'Osteoporosis', 'HighBloodPres', 'HighCholest', 'CognitiveDisorder', 'MacularDegen', 'KidneyDisease', 'LiverDisease']\n",
    "clinical_variables = clinical_variables.assign(comorbidities=clinical_variables[clinical_variables_comorbidities].sum(axis=1))\n",
    "\n",
    "complete_original_dataset = pd.merge(clinical_variables, original_dataset_sid, left_index=True, right_index=True)\n",
    "clinical_variables = complete_original_dataset[clinical_variables.columns]\n",
    "\n",
    "complete_original_dataset['finalgold_visit'].fillna(0, inplace=True)\n",
    "\n",
    "complete_original_dataset.drop(complete_original_dataset[complete_original_dataset['finalgold_visit'] == -1].index, inplace=True)\n",
    "original_dataset_sid = original_dataset_sid[original_dataset_sid.index.isin(complete_original_dataset.index.tolist())]\n",
    "clinical_variables_cols = ['gender', 'age_visit', 'Chronic_Bronchitis', 'PRM_pct_emphysema_Thirona', 'PRM_pct_normal_Thirona', 'Pi10_Thirona', 'comorbidities']\n",
    "\n",
    "graph = nx.from_numpy_array(graph_adj)\n",
    "\n",
    "nodes_features = []\n",
    "for node_name in nodes_names:\n",
    "    node_features = []\n",
    "    for clinical_variable in clinical_variables_cols:\n",
    "        node_features.append(\n",
    "            abs(original_dataset_sid[node_name].corr(complete_original_dataset[clinical_variable].astype('float64'))))\n",
    "    nodes_features.append(node_features)\n",
    "\n",
    "features = np.array(nodes_features)\n",
    "nodes_labels = [abs(x) for x in nodes_correlation_with_phenotype.values.tolist()]\n",
    "graph.remove_edges_from(nx.selfloop_edges(graph))\n",
    "\n",
    "x = np.zeros(features.shape)\n",
    "graph_nodes = list(graph.nodes)\n",
    "for m in range(features.shape[0]):\n",
    "    x[graph_nodes[m]] = features[m]\n",
    "x = torch.from_numpy(x).float()\n",
    "\n",
    "# Edges Indexes\n",
    "edge_index = np.array(list(graph.edges))\n",
    "edge_index = np.concatenate((edge_index, edge_index[:, ::-1]), axis=0)\n",
    "edge_index = torch.from_numpy(edge_index).long().permute(1, 0)\n",
    "\n",
    "# Edges Weights\n",
    "edge_weight = np.array(list(nx.get_edge_attributes(graph, 'weight').values()))\n",
    "edge_weight = np.concatenate((edge_weight, edge_weight), axis=0)\n",
    "edge_weight = torch.from_numpy(edge_weight).float()\n",
    "\n",
    "nodes_labels = torch.from_numpy(np.array(nodes_labels)).float()\n",
    "dataset = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=nodes_labels) # torch.zeros(len(nodes_names)))\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:40:36.079239033Z",
     "start_time": "2024-02-09T05:40:34.641976127Z"
    }
   },
   "id": "679fab2c6c9459ea",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[27, 7], edge_index=[2, 78], edge_attr=[78], y=[27], train_mask=[27], val_mask=[27], test_mask=[27])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = RandomNodeSplit(num_val=5, num_test=5)\n",
    "data = transform(dataset)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:40:36.825819379Z",
     "start_time": "2024-02-09T05:40:36.817978946Z"
    }
   },
   "id": "14911313fac34c27",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        (N(1) + N(8))-acetylspermidine  5-acetylamino-6-amino-3-methyluracil  \\\nsid                                                                            \n10010J                       -0.190253                              2.198677   \n10031R                        1.137203                             -0.556750   \n10032T                       -0.852246                              0.353310   \n10052Z                        0.247807                              0.975486   \n10055F                       -0.366317                             -1.130497   \n...                                ...                                   ...   \n25551W                        0.829446                              0.050146   \n25563D                       -0.209134                             -0.472576   \n25564F                       -0.098917                             -1.349235   \n25571C                       -2.186909                             -0.205182   \n25581F                        2.382749                              1.334424   \n\n        5-hydroxyhexanoate  adrenate (22:4n6)  C-glycosyltryptophan  \\\nsid                                                                   \n10010J            0.951882           1.728713              1.696511   \n10031R            1.417606           0.412620              0.341081   \n10032T            0.415074           0.528622              0.050416   \n10052Z           -1.502064           0.469000              0.333686   \n10055F            0.858598          -0.866032             -0.241559   \n...                    ...                ...                   ...   \n25551W           -2.103680          -0.102467             -0.408373   \n25563D            0.019962          -1.653494             -0.981828   \n25564F            1.152642          -0.118369             -0.103682   \n25571C           -0.552108          -0.625187             -0.446872   \n25581F            0.398210          -0.170399             -0.384768   \n\n        phosphocholine  ergothioneine  myristoleoylcarnitine (C14:1)*  \\\nsid                                                                     \n10010J        1.810291      -1.921257                        0.875180   \n10031R        0.396214      -0.667501                        1.829685   \n10032T        0.134069       0.375303                        0.350066   \n10052Z       -0.163554      -0.732381                        0.195025   \n10055F        0.173388      -0.236713                        1.537696   \n...                ...            ...                             ...   \n25551W       -0.135074       1.477414                       -1.314134   \n25563D        1.120985       0.874699                        0.033871   \n25564F        1.691015      -0.610257                        0.828027   \n25571C        2.273861       0.261602                       -0.991433   \n25581F        0.450509       0.646311                       -0.070302   \n\n        N2,N2-dimethylguanosine  X - 12026  ...  Complement component C9  \\\nsid                                         ...                            \n10010J                 0.892684   0.124223  ...                 0.322382   \n10031R                -0.880821  -1.277666  ...                 0.226270   \n10032T                -2.254200  -0.518312  ...                -0.428367   \n10052Z                -0.199153  -0.187908  ...                -0.409941   \n10055F                -1.105873  -1.534229  ...                 0.660275   \n...                         ...        ...  ...                      ...   \n25551W                 0.006312  -0.615117  ...                -1.925122   \n25563D                -0.957397  -1.584809  ...                 0.294543   \n25564F                 0.085634  -0.500804  ...                -0.173596   \n25571C                -0.777158  -0.851403  ...                 0.294511   \n25581F                -0.306786  -0.151771  ...                -0.016965   \n\n        Carbonic anhydrase 6  Kallistatin  Beta-2-microglobulin  \\\nsid                                                               \n10010J             -1.118447    -0.234189              0.926362   \n10031R             -0.377116    -1.673974              0.100996   \n10032T              0.574186    -0.634489             -0.364988   \n10052Z             -0.775491    -0.197459             -0.731378   \n10055F              0.204040    -0.372867              0.890467   \n...                      ...          ...                   ...   \n25551W             -0.239220     1.567190              0.312708   \n25563D             -0.309081    -0.945435             -0.337557   \n25564F              0.309346    -0.362805             -1.158474   \n25571C             -2.127945     0.022630             -0.590303   \n25581F              1.907504     0.102125             -0.347039   \n\n        C-reactive protein  Growth/differentiation factor 15  \\\nsid                                                            \n10010J            0.557443                          0.017068   \n10031R            0.220795                         -0.145500   \n10032T            0.629139                          0.187559   \n10052Z            1.171704                         -1.459689   \n10055F           -0.852658                          1.371039   \n...                    ...                               ...   \n25551W           -3.290274                         -0.639323   \n25563D            1.224334                         -0.182733   \n25564F            0.221569                         -1.306005   \n25571C            1.132313                         -1.366281   \n25581F           -0.364680                          0.581469   \n\n        Alpha-(1,3)-fucosyltransferase 5  Trefoil factor 3  Troponin T  \\\nsid                                                                      \n10010J                         -0.382598          0.234377    0.593199   \n10031R                         -0.386756         -0.132752   -0.795531   \n10032T                         -1.008337         -0.288907   -0.917175   \n10052Z                          2.014551         -0.646074    0.414110   \n10055F                          1.157176          0.630683    0.051998   \n...                                  ...               ...         ...   \n25551W                         -1.808501         -0.282904   -0.152816   \n25563D                          0.960652         -0.789511   -0.684374   \n25564F                          0.326384         -0.922360   -0.366161   \n25571C                         -0.561698         -0.902983   -0.610586   \n25581F                         -1.911572          0.189512    0.553741   \n\n        N-terminal pro-BNP  \nsid                         \n10010J            1.282038  \n10031R           -0.020434  \n10032T           -0.896100  \n10052Z           -1.828103  \n10055F            2.123272  \n...                    ...  \n25551W            0.776388  \n25563D            1.373968  \n25564F           -1.541756  \n25571C           -0.965605  \n25581F           -1.417684  \n\n[904 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(N(1) + N(8))-acetylspermidine</th>\n      <th>5-acetylamino-6-amino-3-methyluracil</th>\n      <th>5-hydroxyhexanoate</th>\n      <th>adrenate (22:4n6)</th>\n      <th>C-glycosyltryptophan</th>\n      <th>phosphocholine</th>\n      <th>ergothioneine</th>\n      <th>myristoleoylcarnitine (C14:1)*</th>\n      <th>N2,N2-dimethylguanosine</th>\n      <th>X - 12026</th>\n      <th>...</th>\n      <th>Complement component C9</th>\n      <th>Carbonic anhydrase 6</th>\n      <th>Kallistatin</th>\n      <th>Beta-2-microglobulin</th>\n      <th>C-reactive protein</th>\n      <th>Growth/differentiation factor 15</th>\n      <th>Alpha-(1,3)-fucosyltransferase 5</th>\n      <th>Trefoil factor 3</th>\n      <th>Troponin T</th>\n      <th>N-terminal pro-BNP</th>\n    </tr>\n    <tr>\n      <th>sid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10010J</th>\n      <td>-0.190253</td>\n      <td>2.198677</td>\n      <td>0.951882</td>\n      <td>1.728713</td>\n      <td>1.696511</td>\n      <td>1.810291</td>\n      <td>-1.921257</td>\n      <td>0.875180</td>\n      <td>0.892684</td>\n      <td>0.124223</td>\n      <td>...</td>\n      <td>0.322382</td>\n      <td>-1.118447</td>\n      <td>-0.234189</td>\n      <td>0.926362</td>\n      <td>0.557443</td>\n      <td>0.017068</td>\n      <td>-0.382598</td>\n      <td>0.234377</td>\n      <td>0.593199</td>\n      <td>1.282038</td>\n    </tr>\n    <tr>\n      <th>10031R</th>\n      <td>1.137203</td>\n      <td>-0.556750</td>\n      <td>1.417606</td>\n      <td>0.412620</td>\n      <td>0.341081</td>\n      <td>0.396214</td>\n      <td>-0.667501</td>\n      <td>1.829685</td>\n      <td>-0.880821</td>\n      <td>-1.277666</td>\n      <td>...</td>\n      <td>0.226270</td>\n      <td>-0.377116</td>\n      <td>-1.673974</td>\n      <td>0.100996</td>\n      <td>0.220795</td>\n      <td>-0.145500</td>\n      <td>-0.386756</td>\n      <td>-0.132752</td>\n      <td>-0.795531</td>\n      <td>-0.020434</td>\n    </tr>\n    <tr>\n      <th>10032T</th>\n      <td>-0.852246</td>\n      <td>0.353310</td>\n      <td>0.415074</td>\n      <td>0.528622</td>\n      <td>0.050416</td>\n      <td>0.134069</td>\n      <td>0.375303</td>\n      <td>0.350066</td>\n      <td>-2.254200</td>\n      <td>-0.518312</td>\n      <td>...</td>\n      <td>-0.428367</td>\n      <td>0.574186</td>\n      <td>-0.634489</td>\n      <td>-0.364988</td>\n      <td>0.629139</td>\n      <td>0.187559</td>\n      <td>-1.008337</td>\n      <td>-0.288907</td>\n      <td>-0.917175</td>\n      <td>-0.896100</td>\n    </tr>\n    <tr>\n      <th>10052Z</th>\n      <td>0.247807</td>\n      <td>0.975486</td>\n      <td>-1.502064</td>\n      <td>0.469000</td>\n      <td>0.333686</td>\n      <td>-0.163554</td>\n      <td>-0.732381</td>\n      <td>0.195025</td>\n      <td>-0.199153</td>\n      <td>-0.187908</td>\n      <td>...</td>\n      <td>-0.409941</td>\n      <td>-0.775491</td>\n      <td>-0.197459</td>\n      <td>-0.731378</td>\n      <td>1.171704</td>\n      <td>-1.459689</td>\n      <td>2.014551</td>\n      <td>-0.646074</td>\n      <td>0.414110</td>\n      <td>-1.828103</td>\n    </tr>\n    <tr>\n      <th>10055F</th>\n      <td>-0.366317</td>\n      <td>-1.130497</td>\n      <td>0.858598</td>\n      <td>-0.866032</td>\n      <td>-0.241559</td>\n      <td>0.173388</td>\n      <td>-0.236713</td>\n      <td>1.537696</td>\n      <td>-1.105873</td>\n      <td>-1.534229</td>\n      <td>...</td>\n      <td>0.660275</td>\n      <td>0.204040</td>\n      <td>-0.372867</td>\n      <td>0.890467</td>\n      <td>-0.852658</td>\n      <td>1.371039</td>\n      <td>1.157176</td>\n      <td>0.630683</td>\n      <td>0.051998</td>\n      <td>2.123272</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25551W</th>\n      <td>0.829446</td>\n      <td>0.050146</td>\n      <td>-2.103680</td>\n      <td>-0.102467</td>\n      <td>-0.408373</td>\n      <td>-0.135074</td>\n      <td>1.477414</td>\n      <td>-1.314134</td>\n      <td>0.006312</td>\n      <td>-0.615117</td>\n      <td>...</td>\n      <td>-1.925122</td>\n      <td>-0.239220</td>\n      <td>1.567190</td>\n      <td>0.312708</td>\n      <td>-3.290274</td>\n      <td>-0.639323</td>\n      <td>-1.808501</td>\n      <td>-0.282904</td>\n      <td>-0.152816</td>\n      <td>0.776388</td>\n    </tr>\n    <tr>\n      <th>25563D</th>\n      <td>-0.209134</td>\n      <td>-0.472576</td>\n      <td>0.019962</td>\n      <td>-1.653494</td>\n      <td>-0.981828</td>\n      <td>1.120985</td>\n      <td>0.874699</td>\n      <td>0.033871</td>\n      <td>-0.957397</td>\n      <td>-1.584809</td>\n      <td>...</td>\n      <td>0.294543</td>\n      <td>-0.309081</td>\n      <td>-0.945435</td>\n      <td>-0.337557</td>\n      <td>1.224334</td>\n      <td>-0.182733</td>\n      <td>0.960652</td>\n      <td>-0.789511</td>\n      <td>-0.684374</td>\n      <td>1.373968</td>\n    </tr>\n    <tr>\n      <th>25564F</th>\n      <td>-0.098917</td>\n      <td>-1.349235</td>\n      <td>1.152642</td>\n      <td>-0.118369</td>\n      <td>-0.103682</td>\n      <td>1.691015</td>\n      <td>-0.610257</td>\n      <td>0.828027</td>\n      <td>0.085634</td>\n      <td>-0.500804</td>\n      <td>...</td>\n      <td>-0.173596</td>\n      <td>0.309346</td>\n      <td>-0.362805</td>\n      <td>-1.158474</td>\n      <td>0.221569</td>\n      <td>-1.306005</td>\n      <td>0.326384</td>\n      <td>-0.922360</td>\n      <td>-0.366161</td>\n      <td>-1.541756</td>\n    </tr>\n    <tr>\n      <th>25571C</th>\n      <td>-2.186909</td>\n      <td>-0.205182</td>\n      <td>-0.552108</td>\n      <td>-0.625187</td>\n      <td>-0.446872</td>\n      <td>2.273861</td>\n      <td>0.261602</td>\n      <td>-0.991433</td>\n      <td>-0.777158</td>\n      <td>-0.851403</td>\n      <td>...</td>\n      <td>0.294511</td>\n      <td>-2.127945</td>\n      <td>0.022630</td>\n      <td>-0.590303</td>\n      <td>1.132313</td>\n      <td>-1.366281</td>\n      <td>-0.561698</td>\n      <td>-0.902983</td>\n      <td>-0.610586</td>\n      <td>-0.965605</td>\n    </tr>\n    <tr>\n      <th>25581F</th>\n      <td>2.382749</td>\n      <td>1.334424</td>\n      <td>0.398210</td>\n      <td>-0.170399</td>\n      <td>-0.384768</td>\n      <td>0.450509</td>\n      <td>0.646311</td>\n      <td>-0.070302</td>\n      <td>-0.306786</td>\n      <td>-0.151771</td>\n      <td>...</td>\n      <td>-0.016965</td>\n      <td>1.907504</td>\n      <td>0.102125</td>\n      <td>-0.347039</td>\n      <td>-0.364680</td>\n      <td>0.581469</td>\n      <td>-1.911572</td>\n      <td>0.189512</td>\n      <td>0.553741</td>\n      <td>-1.417684</td>\n    </tr>\n  </tbody>\n</table>\n<p>904 rows × 27 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset_sid['finalgold_visit'] = complete_original_dataset['finalgold_visit']\n",
    "original_dataset_sid.drop(['finalgold_visit'], axis=1, inplace=True)\n",
    "original_dataset_sid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T18:26:29.638215426Z",
     "start_time": "2024-02-12T18:26:29.618160263Z"
    }
   },
   "id": "2a8e3a8fddb19b2b",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Unifying Classes 0, -1, {1, 2} -> 1, {3, 4} -> 2\n",
    "complete_original_dataset['finalgold_visit'] = np.where(complete_original_dataset['finalgold_visit'] == 2, 1, complete_original_dataset['finalgold_visit'])\n",
    "complete_original_dataset['finalgold_visit'] = np.where((complete_original_dataset['finalgold_visit'] == 3) |\n",
    "                                                        (complete_original_dataset['finalgold_visit'] == 4), 2, complete_original_dataset['finalgold_visit'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:40:37.676028181Z",
     "start_time": "2024-02-09T05:40:37.669747631Z"
    }
   },
   "id": "4ef4ec3236e57582",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        (N(1) + N(8))-acetylspermidine  5-acetylamino-6-amino-3-methyluracil  \\\nsid                                                                            \n10010J                       -0.190253                              2.198677   \n10031R                        1.137203                             -0.556750   \n10032T                       -0.852246                              0.353310   \n10052Z                        0.247807                              0.975486   \n10055F                       -0.366317                             -1.130497   \n...                                ...                                   ...   \n25551W                        0.829446                              0.050146   \n25563D                       -0.209134                             -0.472576   \n25564F                       -0.098917                             -1.349235   \n25571C                       -2.186909                             -0.205182   \n25581F                        2.382749                              1.334424   \n\n        5-hydroxyhexanoate  adrenate (22:4n6)  C-glycosyltryptophan  \\\nsid                                                                   \n10010J            0.951882           1.728713              1.696511   \n10031R            1.417606           0.412620              0.341081   \n10032T            0.415074           0.528622              0.050416   \n10052Z           -1.502064           0.469000              0.333686   \n10055F            0.858598          -0.866032             -0.241559   \n...                    ...                ...                   ...   \n25551W           -2.103680          -0.102467             -0.408373   \n25563D            0.019962          -1.653494             -0.981828   \n25564F            1.152642          -0.118369             -0.103682   \n25571C           -0.552108          -0.625187             -0.446872   \n25581F            0.398210          -0.170399             -0.384768   \n\n        phosphocholine  ergothioneine  myristoleoylcarnitine (C14:1)*  \\\nsid                                                                     \n10010J        1.810291      -1.921257                        0.875180   \n10031R        0.396214      -0.667501                        1.829685   \n10032T        0.134069       0.375303                        0.350066   \n10052Z       -0.163554      -0.732381                        0.195025   \n10055F        0.173388      -0.236713                        1.537696   \n...                ...            ...                             ...   \n25551W       -0.135074       1.477414                       -1.314134   \n25563D        1.120985       0.874699                        0.033871   \n25564F        1.691015      -0.610257                        0.828027   \n25571C        2.273861       0.261602                       -0.991433   \n25581F        0.450509       0.646311                       -0.070302   \n\n        N2,N2-dimethylguanosine  X - 12026  ...  Carbonic anhydrase 6  \\\nsid                                         ...                         \n10010J                 0.892684   0.124223  ...             -1.118447   \n10031R                -0.880821  -1.277666  ...             -0.377116   \n10032T                -2.254200  -0.518312  ...              0.574186   \n10052Z                -0.199153  -0.187908  ...             -0.775491   \n10055F                -1.105873  -1.534229  ...              0.204040   \n...                         ...        ...  ...                   ...   \n25551W                 0.006312  -0.615117  ...             -0.239220   \n25563D                -0.957397  -1.584809  ...             -0.309081   \n25564F                 0.085634  -0.500804  ...              0.309346   \n25571C                -0.777158  -0.851403  ...             -2.127945   \n25581F                -0.306786  -0.151771  ...              1.907504   \n\n        Kallistatin  Beta-2-microglobulin  C-reactive protein  \\\nsid                                                             \n10010J    -0.234189              0.926362            0.557443   \n10031R    -1.673974              0.100996            0.220795   \n10032T    -0.634489             -0.364988            0.629139   \n10052Z    -0.197459             -0.731378            1.171704   \n10055F    -0.372867              0.890467           -0.852658   \n...             ...                   ...                 ...   \n25551W     1.567190              0.312708           -3.290274   \n25563D    -0.945435             -0.337557            1.224334   \n25564F    -0.362805             -1.158474            0.221569   \n25571C     0.022630             -0.590303            1.132313   \n25581F     0.102125             -0.347039           -0.364680   \n\n        Growth/differentiation factor 15  Alpha-(1,3)-fucosyltransferase 5  \\\nsid                                                                          \n10010J                          0.017068                         -0.382598   \n10031R                         -0.145500                         -0.386756   \n10032T                          0.187559                         -1.008337   \n10052Z                         -1.459689                          2.014551   \n10055F                          1.371039                          1.157176   \n...                                  ...                               ...   \n25551W                         -0.639323                         -1.808501   \n25563D                         -0.182733                          0.960652   \n25564F                         -1.306005                          0.326384   \n25571C                         -1.366281                         -0.561698   \n25581F                          0.581469                         -1.911572   \n\n        Trefoil factor 3  Troponin T  N-terminal pro-BNP  finalgold_visit  \nsid                                                                        \n10010J          0.234377    0.593199            1.282038              1.0  \n10031R         -0.132752   -0.795531           -0.020434              1.0  \n10032T         -0.288907   -0.917175           -0.896100              1.0  \n10052Z         -0.646074    0.414110           -1.828103              2.0  \n10055F          0.630683    0.051998            2.123272              0.0  \n...                  ...         ...                 ...              ...  \n25551W         -0.282904   -0.152816            0.776388              0.0  \n25563D         -0.789511   -0.684374            1.373968              0.0  \n25564F         -0.922360   -0.366161           -1.541756              0.0  \n25571C         -0.902983   -0.610586           -0.965605              0.0  \n25581F          0.189512    0.553741           -1.417684              0.0  \n\n[904 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(N(1) + N(8))-acetylspermidine</th>\n      <th>5-acetylamino-6-amino-3-methyluracil</th>\n      <th>5-hydroxyhexanoate</th>\n      <th>adrenate (22:4n6)</th>\n      <th>C-glycosyltryptophan</th>\n      <th>phosphocholine</th>\n      <th>ergothioneine</th>\n      <th>myristoleoylcarnitine (C14:1)*</th>\n      <th>N2,N2-dimethylguanosine</th>\n      <th>X - 12026</th>\n      <th>...</th>\n      <th>Carbonic anhydrase 6</th>\n      <th>Kallistatin</th>\n      <th>Beta-2-microglobulin</th>\n      <th>C-reactive protein</th>\n      <th>Growth/differentiation factor 15</th>\n      <th>Alpha-(1,3)-fucosyltransferase 5</th>\n      <th>Trefoil factor 3</th>\n      <th>Troponin T</th>\n      <th>N-terminal pro-BNP</th>\n      <th>finalgold_visit</th>\n    </tr>\n    <tr>\n      <th>sid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10010J</th>\n      <td>-0.190253</td>\n      <td>2.198677</td>\n      <td>0.951882</td>\n      <td>1.728713</td>\n      <td>1.696511</td>\n      <td>1.810291</td>\n      <td>-1.921257</td>\n      <td>0.875180</td>\n      <td>0.892684</td>\n      <td>0.124223</td>\n      <td>...</td>\n      <td>-1.118447</td>\n      <td>-0.234189</td>\n      <td>0.926362</td>\n      <td>0.557443</td>\n      <td>0.017068</td>\n      <td>-0.382598</td>\n      <td>0.234377</td>\n      <td>0.593199</td>\n      <td>1.282038</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10031R</th>\n      <td>1.137203</td>\n      <td>-0.556750</td>\n      <td>1.417606</td>\n      <td>0.412620</td>\n      <td>0.341081</td>\n      <td>0.396214</td>\n      <td>-0.667501</td>\n      <td>1.829685</td>\n      <td>-0.880821</td>\n      <td>-1.277666</td>\n      <td>...</td>\n      <td>-0.377116</td>\n      <td>-1.673974</td>\n      <td>0.100996</td>\n      <td>0.220795</td>\n      <td>-0.145500</td>\n      <td>-0.386756</td>\n      <td>-0.132752</td>\n      <td>-0.795531</td>\n      <td>-0.020434</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10032T</th>\n      <td>-0.852246</td>\n      <td>0.353310</td>\n      <td>0.415074</td>\n      <td>0.528622</td>\n      <td>0.050416</td>\n      <td>0.134069</td>\n      <td>0.375303</td>\n      <td>0.350066</td>\n      <td>-2.254200</td>\n      <td>-0.518312</td>\n      <td>...</td>\n      <td>0.574186</td>\n      <td>-0.634489</td>\n      <td>-0.364988</td>\n      <td>0.629139</td>\n      <td>0.187559</td>\n      <td>-1.008337</td>\n      <td>-0.288907</td>\n      <td>-0.917175</td>\n      <td>-0.896100</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10052Z</th>\n      <td>0.247807</td>\n      <td>0.975486</td>\n      <td>-1.502064</td>\n      <td>0.469000</td>\n      <td>0.333686</td>\n      <td>-0.163554</td>\n      <td>-0.732381</td>\n      <td>0.195025</td>\n      <td>-0.199153</td>\n      <td>-0.187908</td>\n      <td>...</td>\n      <td>-0.775491</td>\n      <td>-0.197459</td>\n      <td>-0.731378</td>\n      <td>1.171704</td>\n      <td>-1.459689</td>\n      <td>2.014551</td>\n      <td>-0.646074</td>\n      <td>0.414110</td>\n      <td>-1.828103</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>10055F</th>\n      <td>-0.366317</td>\n      <td>-1.130497</td>\n      <td>0.858598</td>\n      <td>-0.866032</td>\n      <td>-0.241559</td>\n      <td>0.173388</td>\n      <td>-0.236713</td>\n      <td>1.537696</td>\n      <td>-1.105873</td>\n      <td>-1.534229</td>\n      <td>...</td>\n      <td>0.204040</td>\n      <td>-0.372867</td>\n      <td>0.890467</td>\n      <td>-0.852658</td>\n      <td>1.371039</td>\n      <td>1.157176</td>\n      <td>0.630683</td>\n      <td>0.051998</td>\n      <td>2.123272</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25551W</th>\n      <td>0.829446</td>\n      <td>0.050146</td>\n      <td>-2.103680</td>\n      <td>-0.102467</td>\n      <td>-0.408373</td>\n      <td>-0.135074</td>\n      <td>1.477414</td>\n      <td>-1.314134</td>\n      <td>0.006312</td>\n      <td>-0.615117</td>\n      <td>...</td>\n      <td>-0.239220</td>\n      <td>1.567190</td>\n      <td>0.312708</td>\n      <td>-3.290274</td>\n      <td>-0.639323</td>\n      <td>-1.808501</td>\n      <td>-0.282904</td>\n      <td>-0.152816</td>\n      <td>0.776388</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25563D</th>\n      <td>-0.209134</td>\n      <td>-0.472576</td>\n      <td>0.019962</td>\n      <td>-1.653494</td>\n      <td>-0.981828</td>\n      <td>1.120985</td>\n      <td>0.874699</td>\n      <td>0.033871</td>\n      <td>-0.957397</td>\n      <td>-1.584809</td>\n      <td>...</td>\n      <td>-0.309081</td>\n      <td>-0.945435</td>\n      <td>-0.337557</td>\n      <td>1.224334</td>\n      <td>-0.182733</td>\n      <td>0.960652</td>\n      <td>-0.789511</td>\n      <td>-0.684374</td>\n      <td>1.373968</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25564F</th>\n      <td>-0.098917</td>\n      <td>-1.349235</td>\n      <td>1.152642</td>\n      <td>-0.118369</td>\n      <td>-0.103682</td>\n      <td>1.691015</td>\n      <td>-0.610257</td>\n      <td>0.828027</td>\n      <td>0.085634</td>\n      <td>-0.500804</td>\n      <td>...</td>\n      <td>0.309346</td>\n      <td>-0.362805</td>\n      <td>-1.158474</td>\n      <td>0.221569</td>\n      <td>-1.306005</td>\n      <td>0.326384</td>\n      <td>-0.922360</td>\n      <td>-0.366161</td>\n      <td>-1.541756</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25571C</th>\n      <td>-2.186909</td>\n      <td>-0.205182</td>\n      <td>-0.552108</td>\n      <td>-0.625187</td>\n      <td>-0.446872</td>\n      <td>2.273861</td>\n      <td>0.261602</td>\n      <td>-0.991433</td>\n      <td>-0.777158</td>\n      <td>-0.851403</td>\n      <td>...</td>\n      <td>-2.127945</td>\n      <td>0.022630</td>\n      <td>-0.590303</td>\n      <td>1.132313</td>\n      <td>-1.366281</td>\n      <td>-0.561698</td>\n      <td>-0.902983</td>\n      <td>-0.610586</td>\n      <td>-0.965605</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25581F</th>\n      <td>2.382749</td>\n      <td>1.334424</td>\n      <td>0.398210</td>\n      <td>-0.170399</td>\n      <td>-0.384768</td>\n      <td>0.450509</td>\n      <td>0.646311</td>\n      <td>-0.070302</td>\n      <td>-0.306786</td>\n      <td>-0.151771</td>\n      <td>...</td>\n      <td>1.907504</td>\n      <td>0.102125</td>\n      <td>-0.347039</td>\n      <td>-0.364680</td>\n      <td>0.581469</td>\n      <td>-1.911572</td>\n      <td>0.189512</td>\n      <td>0.553741</td>\n      <td>-1.417684</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>904 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_save = original_dataset_sid\n",
    "dataset_to_save['finalgold_visit'] = complete_original_dataset['finalgold_visit']\n",
    "additional_100_samples = dataset_to_save.sample(n=100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T20:43:31.687866353Z",
     "start_time": "2024-02-06T20:43:31.675401079Z"
    }
   },
   "id": "e22f1cfb2ac944a5",
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      (N(1) + N(8))-acetylspermidine  5-acetylamino-6-amino-3-methyluracil  \\\n0                          -0.190253                              2.198677   \n1                           1.137203                             -0.556750   \n2                          -0.852246                              0.353310   \n3                           0.247807                              0.975486   \n4                          -0.366317                             -1.130497   \n...                              ...                                   ...   \n999                         0.659278                             -1.946808   \n1000                       -0.642646                             -0.705518   \n1001                        1.855791                              0.055643   \n1002                       -1.707297                             -0.743802   \n1003                        2.382749                              1.334424   \n\n      5-hydroxyhexanoate  adrenate (22:4n6)  C-glycosyltryptophan  \\\n0               0.951882           1.728713              1.696511   \n1               1.417606           0.412620              0.341081   \n2               0.415074           0.528622              0.050416   \n3              -1.502064           0.469000              0.333686   \n4               0.858598          -0.866032             -0.241559   \n...                  ...                ...                   ...   \n999            -0.669276          -0.096424             -0.720189   \n1000           -0.616199          -0.780682              0.009038   \n1001           -0.198139          -1.327981             -0.685873   \n1002           -0.170627           0.745788             -0.739919   \n1003            0.398210          -0.170399             -0.384768   \n\n      phosphocholine  ergothioneine  myristoleoylcarnitine (C14:1)*  \\\n0           1.810291      -1.921257                        0.875180   \n1           0.396214      -0.667501                        1.829685   \n2           0.134069       0.375303                        0.350066   \n3          -0.163554      -0.732381                        0.195025   \n4           0.173388      -0.236713                        1.537696   \n...              ...            ...                             ...   \n999        -0.871795      -0.931859                       -0.699814   \n1000        0.139536       0.690948                       -0.929369   \n1001        0.710818       2.217788                        0.675797   \n1002       -0.352481      -2.994006                       -0.717883   \n1003        0.450509       0.646311                       -0.070302   \n\n      N2,N2-dimethylguanosine  X - 12026  ...  Carbonic anhydrase 6  \\\n0                    0.892684   0.124223  ...             -1.118447   \n1                   -0.880821  -1.277666  ...             -0.377116   \n2                   -2.254200  -0.518312  ...              0.574186   \n3                   -0.199153  -0.187908  ...             -0.775491   \n4                   -1.105873  -1.534229  ...              0.204040   \n...                       ...        ...  ...                   ...   \n999                 -0.660706  -1.673829  ...             -0.547514   \n1000                -0.061987   0.062773  ...              1.089632   \n1001                 0.321138   0.948820  ...              2.216299   \n1002                -0.347868   0.572050  ...              1.357573   \n1003                -0.306786  -0.151771  ...              1.907504   \n\n      Kallistatin  Beta-2-microglobulin  C-reactive protein  \\\n0       -0.234189              0.926362            0.557443   \n1       -1.673974              0.100996            0.220795   \n2       -0.634489             -0.364988            0.629139   \n3       -0.197459             -0.731378            1.171704   \n4       -0.372867              0.890467           -0.852658   \n...           ...                   ...                 ...   \n999     -0.272055             -0.321034           -0.286900   \n1000    -0.693704             -0.392294           -0.084729   \n1001     0.737917             -1.432378            0.033276   \n1002     0.909138              0.075615            0.007780   \n1003     0.102125             -0.347039           -0.364680   \n\n      Growth/differentiation factor 15  Alpha-(1,3)-fucosyltransferase 5  \\\n0                             0.017068                         -0.382598   \n1                            -0.145500                         -0.386756   \n2                             0.187559                         -1.008337   \n3                            -1.459689                          2.014551   \n4                             1.371039                          1.157176   \n...                                ...                               ...   \n999                          -0.391613                         -1.214222   \n1000                         -0.509537                          0.512960   \n1001                         -0.812547                          0.827589   \n1002                          1.562260                         -0.538427   \n1003                          0.581469                         -1.911572   \n\n      Trefoil factor 3  Troponin T  N-terminal pro-BNP  finalgold_visit  \n0             0.234377    0.593199            1.282038              1.0  \n1            -0.132752   -0.795531           -0.020434              1.0  \n2            -0.288907   -0.917175           -0.896100              1.0  \n3            -0.646074    0.414110           -1.828103              2.0  \n4             0.630683    0.051998            2.123272              0.0  \n...                ...         ...                 ...              ...  \n999          -1.845630   -0.256115           -1.807875              0.0  \n1000         -0.751631   -0.731859           -0.257789              2.0  \n1001         -1.083431   -0.406224            0.667079              0.0  \n1002         -0.361136    0.534807           -0.756106              0.0  \n1003          0.189512    0.553741           -1.417684              0.0  \n\n[1004 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(N(1) + N(8))-acetylspermidine</th>\n      <th>5-acetylamino-6-amino-3-methyluracil</th>\n      <th>5-hydroxyhexanoate</th>\n      <th>adrenate (22:4n6)</th>\n      <th>C-glycosyltryptophan</th>\n      <th>phosphocholine</th>\n      <th>ergothioneine</th>\n      <th>myristoleoylcarnitine (C14:1)*</th>\n      <th>N2,N2-dimethylguanosine</th>\n      <th>X - 12026</th>\n      <th>...</th>\n      <th>Carbonic anhydrase 6</th>\n      <th>Kallistatin</th>\n      <th>Beta-2-microglobulin</th>\n      <th>C-reactive protein</th>\n      <th>Growth/differentiation factor 15</th>\n      <th>Alpha-(1,3)-fucosyltransferase 5</th>\n      <th>Trefoil factor 3</th>\n      <th>Troponin T</th>\n      <th>N-terminal pro-BNP</th>\n      <th>finalgold_visit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.190253</td>\n      <td>2.198677</td>\n      <td>0.951882</td>\n      <td>1.728713</td>\n      <td>1.696511</td>\n      <td>1.810291</td>\n      <td>-1.921257</td>\n      <td>0.875180</td>\n      <td>0.892684</td>\n      <td>0.124223</td>\n      <td>...</td>\n      <td>-1.118447</td>\n      <td>-0.234189</td>\n      <td>0.926362</td>\n      <td>0.557443</td>\n      <td>0.017068</td>\n      <td>-0.382598</td>\n      <td>0.234377</td>\n      <td>0.593199</td>\n      <td>1.282038</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.137203</td>\n      <td>-0.556750</td>\n      <td>1.417606</td>\n      <td>0.412620</td>\n      <td>0.341081</td>\n      <td>0.396214</td>\n      <td>-0.667501</td>\n      <td>1.829685</td>\n      <td>-0.880821</td>\n      <td>-1.277666</td>\n      <td>...</td>\n      <td>-0.377116</td>\n      <td>-1.673974</td>\n      <td>0.100996</td>\n      <td>0.220795</td>\n      <td>-0.145500</td>\n      <td>-0.386756</td>\n      <td>-0.132752</td>\n      <td>-0.795531</td>\n      <td>-0.020434</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.852246</td>\n      <td>0.353310</td>\n      <td>0.415074</td>\n      <td>0.528622</td>\n      <td>0.050416</td>\n      <td>0.134069</td>\n      <td>0.375303</td>\n      <td>0.350066</td>\n      <td>-2.254200</td>\n      <td>-0.518312</td>\n      <td>...</td>\n      <td>0.574186</td>\n      <td>-0.634489</td>\n      <td>-0.364988</td>\n      <td>0.629139</td>\n      <td>0.187559</td>\n      <td>-1.008337</td>\n      <td>-0.288907</td>\n      <td>-0.917175</td>\n      <td>-0.896100</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.247807</td>\n      <td>0.975486</td>\n      <td>-1.502064</td>\n      <td>0.469000</td>\n      <td>0.333686</td>\n      <td>-0.163554</td>\n      <td>-0.732381</td>\n      <td>0.195025</td>\n      <td>-0.199153</td>\n      <td>-0.187908</td>\n      <td>...</td>\n      <td>-0.775491</td>\n      <td>-0.197459</td>\n      <td>-0.731378</td>\n      <td>1.171704</td>\n      <td>-1.459689</td>\n      <td>2.014551</td>\n      <td>-0.646074</td>\n      <td>0.414110</td>\n      <td>-1.828103</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.366317</td>\n      <td>-1.130497</td>\n      <td>0.858598</td>\n      <td>-0.866032</td>\n      <td>-0.241559</td>\n      <td>0.173388</td>\n      <td>-0.236713</td>\n      <td>1.537696</td>\n      <td>-1.105873</td>\n      <td>-1.534229</td>\n      <td>...</td>\n      <td>0.204040</td>\n      <td>-0.372867</td>\n      <td>0.890467</td>\n      <td>-0.852658</td>\n      <td>1.371039</td>\n      <td>1.157176</td>\n      <td>0.630683</td>\n      <td>0.051998</td>\n      <td>2.123272</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>0.659278</td>\n      <td>-1.946808</td>\n      <td>-0.669276</td>\n      <td>-0.096424</td>\n      <td>-0.720189</td>\n      <td>-0.871795</td>\n      <td>-0.931859</td>\n      <td>-0.699814</td>\n      <td>-0.660706</td>\n      <td>-1.673829</td>\n      <td>...</td>\n      <td>-0.547514</td>\n      <td>-0.272055</td>\n      <td>-0.321034</td>\n      <td>-0.286900</td>\n      <td>-0.391613</td>\n      <td>-1.214222</td>\n      <td>-1.845630</td>\n      <td>-0.256115</td>\n      <td>-1.807875</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>-0.642646</td>\n      <td>-0.705518</td>\n      <td>-0.616199</td>\n      <td>-0.780682</td>\n      <td>0.009038</td>\n      <td>0.139536</td>\n      <td>0.690948</td>\n      <td>-0.929369</td>\n      <td>-0.061987</td>\n      <td>0.062773</td>\n      <td>...</td>\n      <td>1.089632</td>\n      <td>-0.693704</td>\n      <td>-0.392294</td>\n      <td>-0.084729</td>\n      <td>-0.509537</td>\n      <td>0.512960</td>\n      <td>-0.751631</td>\n      <td>-0.731859</td>\n      <td>-0.257789</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1001</th>\n      <td>1.855791</td>\n      <td>0.055643</td>\n      <td>-0.198139</td>\n      <td>-1.327981</td>\n      <td>-0.685873</td>\n      <td>0.710818</td>\n      <td>2.217788</td>\n      <td>0.675797</td>\n      <td>0.321138</td>\n      <td>0.948820</td>\n      <td>...</td>\n      <td>2.216299</td>\n      <td>0.737917</td>\n      <td>-1.432378</td>\n      <td>0.033276</td>\n      <td>-0.812547</td>\n      <td>0.827589</td>\n      <td>-1.083431</td>\n      <td>-0.406224</td>\n      <td>0.667079</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1002</th>\n      <td>-1.707297</td>\n      <td>-0.743802</td>\n      <td>-0.170627</td>\n      <td>0.745788</td>\n      <td>-0.739919</td>\n      <td>-0.352481</td>\n      <td>-2.994006</td>\n      <td>-0.717883</td>\n      <td>-0.347868</td>\n      <td>0.572050</td>\n      <td>...</td>\n      <td>1.357573</td>\n      <td>0.909138</td>\n      <td>0.075615</td>\n      <td>0.007780</td>\n      <td>1.562260</td>\n      <td>-0.538427</td>\n      <td>-0.361136</td>\n      <td>0.534807</td>\n      <td>-0.756106</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1003</th>\n      <td>2.382749</td>\n      <td>1.334424</td>\n      <td>0.398210</td>\n      <td>-0.170399</td>\n      <td>-0.384768</td>\n      <td>0.450509</td>\n      <td>0.646311</td>\n      <td>-0.070302</td>\n      <td>-0.306786</td>\n      <td>-0.151771</td>\n      <td>...</td>\n      <td>1.907504</td>\n      <td>0.102125</td>\n      <td>-0.347039</td>\n      <td>-0.364680</td>\n      <td>0.581469</td>\n      <td>-1.911572</td>\n      <td>0.189512</td>\n      <td>0.553741</td>\n      <td>-1.417684</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1004 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_100_samples = dataset_to_save.sample(n=100)\n",
    "dataset_to_save = pd.concat([dataset_to_save, additional_100_samples], ignore_index=True)\n",
    "dataset_to_save"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T21:02:33.403052396Z",
     "start_time": "2024-02-06T21:02:33.388654203Z"
    }
   },
   "id": "d808dda2c152fd67",
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "dataset_to_save = dataset_to_save.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "dataset_to_save.to_csv(\"OriginalDatasetwithGOLD.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T02:01:51.838949967Z",
     "start_time": "2024-02-07T02:01:51.813603439Z"
    }
   },
   "id": "8484efc1cbc2287d",
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "def embedding_autoencoder(nodes_embeddings):\n",
    "    # Define the autoencoder model\n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self, input_dim, encoding_dim):\n",
    "            super(Autoencoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, 8),   # Input layer -> Hidden layer 1\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, 4),           # Hidden layer 1 -> Hidden layer 2\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4, encoding_dim) # Hidden layer 2 -> Encoding layer\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(encoding_dim, 4), # Encoding layer -> Hidden layer 2\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4, 8),            # Hidden layer 2 -> Hidden layer 1\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, input_dim)     # Hidden layer 1 -> Output layer\n",
    "            )\n",
    "    \n",
    "        def forward(self, x):\n",
    "            x = self.encoder(x)\n",
    "            x = self.decoder(x)\n",
    "            return x\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Define hyperparameters\n",
    "    input_dim = nodes_embeddings.shape[1]\n",
    "    encoding_dim = 1\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 300\n",
    "    \n",
    "    # Create the autoencoder model\n",
    "    model = Autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        output = model(nodes_embeddings)\n",
    "        loss = criterion(output, nodes_embeddings)\n",
    "    \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # # Print progress\n",
    "        # if (epoch+1) % 10 == 0:\n",
    "        #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # # Test the autoencoder\n",
    "    # test_input = torch.randn(1, input_dim)\n",
    "    # encoded_output = model.encoder(test_input)\n",
    "    # decoded_output = model.decoder(encoded_output)\n",
    "    # \n",
    "    # print(f'Original input: {test_input}')\n",
    "    # print(f'Encoded output: {encoded_output}')\n",
    "    # print(f'Decoded output: {decoded_output}')\n",
    "    \n",
    "    encoded_output = model.encoder(nodes_embeddings)\n",
    "    # print(\"Encoded Output %s\" % encoded_output)\n",
    "    return encoded_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:40:45.167157251Z",
     "start_time": "2024-02-09T05:40:45.159448995Z"
    }
   },
   "id": "635e4643b1c0afd",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 0.0736\n",
      "Epoch [20/200], Loss: 0.0651\n",
      "Epoch [30/200], Loss: 0.0577\n",
      "Epoch [40/200], Loss: 0.0513\n",
      "Epoch [50/200], Loss: 0.0458\n",
      "Epoch [60/200], Loss: 0.0410\n",
      "Epoch [70/200], Loss: 0.0369\n",
      "Epoch [80/200], Loss: 0.0333\n",
      "Epoch [90/200], Loss: 0.0301\n",
      "Epoch [100/200], Loss: 0.0274\n",
      "Epoch [110/200], Loss: 0.0250\n",
      "Epoch [120/200], Loss: 0.0229\n",
      "Epoch [130/200], Loss: 0.0210\n",
      "Epoch [140/200], Loss: 0.0193\n",
      "Epoch [150/200], Loss: 0.0175\n",
      "Epoch [160/200], Loss: 0.0156\n",
      "Epoch [170/200], Loss: 0.0137\n",
      "Epoch [180/200], Loss: 0.0119\n",
      "Epoch [190/200], Loss: 0.0103\n",
      "Epoch [200/200], Loss: 0.0089\n"
     ]
    },
    {
     "data": {
      "text/plain": "-0.6838477849960327"
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph_nodes_embeddings = pd.read_csv('HiddenLayerOutput_GCN_trimmed_fev1_0.515_0.111_adj.csv')\n",
    "subgraph_nodes_embeddings = torch.tensor(subgraph_nodes_embeddings.values, dtype=torch.float32)\n",
    "t = embedding_autoencoder(subgraph_nodes_embeddings)\n",
    "t.detach().numpy().tolist()[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T18:03:23.531982043Z",
     "start_time": "2024-02-08T18:03:22.935150765Z"
    }
   },
   "id": "bf4c6868a53d8b19",
   "execution_count": 229
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def integrate_final_embeddings():\n",
    "    # Integrating Network Information (from Embeddings) and Attempting to Predict the Phenotype\n",
    "    subgraph_adj = pd.read_csv('../GNNs/data/COPD/SparsifiedNetworks/trimmed_fev1_0.515_0.111_adj.csv', index_col=0).to_numpy()\n",
    "    subgraph = nx.from_numpy_array(subgraph_adj)\n",
    "\n",
    "    subgraph_nodes_dict = {}\n",
    "    for subgraph_node in subgraph.nodes():\n",
    "        subgraph_node_name = original_dataset.iloc[:0, subgraph_node].name\n",
    "        subgraph_nodes_dict[subgraph_node] = subgraph_node_name\n",
    "\n",
    "    subgraph_nodes_embeddings = pd.read_csv('HiddenLayerOutput_GCN_trimmed_fev1_0.515_0.111_adj.csv')\n",
    "    embeddings_dim = subgraph_nodes_embeddings.shape[1]\n",
    "    # subgraph_nodes_embeddings = subgraph_nodes_embeddings.T\n",
    "    \n",
    "    # Aggregating dimensions of the embedding space\n",
    "    # subgraph_nodes_embeddings = subgraph_nodes_embeddings.mean(axis=0)\n",
    "    # subgraph_nodes_embeddings = subgraph_nodes_embeddings.max(axis=0)\n",
    "    # pca = PCA()\n",
    "    # pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "    # Xt = pca.fit_transform(subgraph_nodes_embeddings)\n",
    "    # explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    # subgraph_nodes_embeddings = Xt[:,0]\n",
    "    # \n",
    "    # print(\"explained variance ratio %s\" % explained_variance_ratio)\n",
    "    # print(subgraph_nodes_embeddings[0])\n",
    "    \n",
    "    subgraph_nodes_embeddings = embedding_autoencoder(torch.tensor(subgraph_nodes_embeddings.values, dtype=torch.float32))\n",
    "    subgraph_nodes_embeddings = subgraph_nodes_embeddings.detach().numpy().tolist()\n",
    "    \n",
    "    subgraph_nodes_dict_inv = dict((v, k) for k, v in subgraph_nodes_dict.items())\n",
    "    original_dataset_with_embeddings_df = pd.DataFrame(index=range(0, len(original_dataset_sid)))\n",
    "    columns_names = original_dataset_sid.columns\n",
    "\n",
    "    for column_name in columns_names:\n",
    "        # Get the Metabolite or Protein Embedding Created using Node2Vec\n",
    "        subgraph_node_embedding = subgraph_nodes_embeddings[subgraph_nodes_dict_inv[column_name]][0]\n",
    "        # Attempting to Exaggerate Differences in Nodes 14 and 25 (Hub Nodes)\n",
    "        # if subgraph_nodes_dict_inv[column_name] in [14, 25]:\n",
    "        #     subgraph_node_embedding = subgraph_node_embedding * 100\n",
    "        # print(len(subgraph_node_embedding))\n",
    "        column_value = original_dataset_sid[column_name]\n",
    "        z = [[vy * subgraph_node_embedding] for _, vy in enumerate(column_value)]\n",
    "        original_dataset_with_embeddings_df = pd.concat([original_dataset_with_embeddings_df, pd.DataFrame(z)], axis=1)\n",
    "\n",
    "    # original_dataset_with_embeddings_df.columns = range(0, len(original_dataset_sid.columns)*embeddings_dim)\n",
    "    original_dataset_with_embeddings_df.columns = original_dataset_sid.columns\n",
    "\n",
    "    # i = 0\n",
    "    # original_dataset_with_embeddings_df_sliced = []\n",
    "    # while i < len(subgraph_node_embedding):\n",
    "    #     tmp = original_dataset_with_embeddings_df[[x for x in range(i, len(original_dataset_with_embeddings_df.columns), len(subgraph_node_embedding))]]\n",
    "    #     tmp.index = original_dataset_sid.index\n",
    "    #     original_dataset_with_embeddings_df_sliced.append(tmp)\n",
    "    #     i += 1\n",
    "\n",
    "    # for dim_idx, original_dataset_with_embeddings in enumerate(original_dataset_with_embeddings_df_sliced):\n",
    "    #     # print(\"****** Dim %d\" % dim_idx)\n",
    "    #     original_dataset_with_embeddings_new = pd.DataFrame(original_dataset_with_embeddings)\n",
    "    #     if dim_idx == 10: break\n",
    "    # return original_dataset_with_embeddings_df_sliced\n",
    "    return original_dataset_with_embeddings_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:40:49.747040730Z",
     "start_time": "2024-02-09T05:40:49.744661531Z"
    }
   },
   "id": "953eeb6d1ade967f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     (N(1) + N(8))-acetylspermidine  5-acetylamino-6-amino-3-methyluracil  \\\n0                          0.020155                              0.085807   \n1                         -0.120475                             -0.021728   \n2                          0.090287                              0.013788   \n3                         -0.026253                              0.038070   \n4                          0.038808                             -0.044119   \n..                              ...                                   ...   \n899                       -0.087872                              0.001957   \n900                        0.022156                             -0.018443   \n901                        0.010479                             -0.052656   \n902                        0.231681                             -0.008008   \n903                       -0.252429                              0.052078   \n\n     5-hydroxyhexanoate  adrenate (22:4n6)  C-glycosyltryptophan  \\\n0             -0.137053           0.060389              0.127048   \n1             -0.204108           0.014414              0.025543   \n2             -0.059763           0.018466              0.003776   \n3              0.216268           0.016384              0.024989   \n4             -0.123622          -0.030253             -0.018090   \n..                  ...                ...                   ...   \n899            0.302890          -0.003579             -0.030582   \n900           -0.002874          -0.057762             -0.073527   \n901           -0.165958          -0.004135             -0.007765   \n902            0.079493          -0.021840             -0.033465   \n903           -0.057335          -0.005953             -0.028814   \n\n     phosphocholine  ergothioneine  myristoleoylcarnitine (C14:1)*  \\\n0         -0.241752       0.358008                       -0.128705   \n1         -0.052912       0.124383                       -0.269076   \n2         -0.017904      -0.069934                       -0.051481   \n3          0.021841       0.136472                       -0.028681   \n4         -0.023155       0.044109                       -0.226136   \n..              ...            ...                             ...   \n899        0.018038      -0.275302                        0.193258   \n900       -0.149700      -0.162992                       -0.004981   \n901       -0.225824       0.113716                       -0.121771   \n902       -0.303659      -0.048747                        0.145802   \n903       -0.060162      -0.120434                        0.010339   \n\n     N2,N2-dimethylguanosine  X - 12026  ...  Complement component C9  \\\n0                   0.076749   0.010245  ...                -0.038307   \n1                  -0.075729  -0.105374  ...                -0.026887   \n2                  -0.193806  -0.042747  ...                 0.050901   \n3                  -0.017122  -0.015498  ...                 0.048712   \n4                  -0.095078  -0.126534  ...                -0.078458   \n..                       ...        ...  ...                      ...   \n899                 0.000543  -0.050731  ...                 0.228754   \n900                -0.082313  -0.130706  ...                -0.034999   \n901                 0.007362  -0.041303  ...                 0.020628   \n902                -0.066817  -0.070219  ...                -0.034996   \n903                -0.026376  -0.012517  ...                 0.002016   \n\n     Carbonic anhydrase 6  Kallistatin  Beta-2-microglobulin  \\\n0                0.189493     0.034384              0.110211   \n1                0.063893     0.245779              0.012016   \n2               -0.097281     0.093158             -0.043423   \n3                0.131387     0.028992             -0.087013   \n4               -0.034569     0.054746              0.105940   \n..                    ...          ...                   ...   \n899              0.040530    -0.230100              0.037203   \n900              0.052366     0.138812             -0.040160   \n901             -0.052411     0.053268             -0.137826   \n902              0.360527    -0.003323             -0.070229   \n903             -0.323179    -0.014994             -0.041288   \n\n     C-reactive protein  Growth/differentiation factor 15  \\\n0             -0.099851                          0.001265   \n1             -0.039549                         -0.010779   \n2             -0.112693                          0.013895   \n3             -0.209878                         -0.108141   \n4              0.152730                          0.101574   \n..                  ...                               ...   \n899            0.589362                         -0.047364   \n900           -0.219306                         -0.013538   \n901           -0.039688                         -0.096756   \n902           -0.202823                         -0.101221   \n903            0.065322                          0.043078   \n\n     Alpha-(1,3)-fucosyltransferase 5  Trefoil factor 3  Troponin T  \\\n0                            0.036117          0.006813    0.251452   \n1                            0.036510         -0.003859   -0.337219   \n2                            0.095186         -0.008398   -0.388783   \n3                           -0.190172         -0.018780    0.175538   \n4                           -0.109237          0.018332    0.022042   \n..                                ...               ...         ...   \n899                          0.170722         -0.008223   -0.064778   \n900                         -0.090685         -0.022949   -0.290100   \n901                         -0.030811         -0.026811   -0.155213   \n902                          0.053024         -0.026247   -0.258822   \n903                          0.180451          0.005509    0.234726   \n\n     N-terminal pro-BNP  \n0              0.160592  \n1             -0.002560  \n2             -0.112248  \n3             -0.228993  \n4              0.265967  \n..                  ...  \n899            0.097253  \n900            0.172107  \n901           -0.193125  \n902           -0.120955  \n903           -0.177583  \n\n[904 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(N(1) + N(8))-acetylspermidine</th>\n      <th>5-acetylamino-6-amino-3-methyluracil</th>\n      <th>5-hydroxyhexanoate</th>\n      <th>adrenate (22:4n6)</th>\n      <th>C-glycosyltryptophan</th>\n      <th>phosphocholine</th>\n      <th>ergothioneine</th>\n      <th>myristoleoylcarnitine (C14:1)*</th>\n      <th>N2,N2-dimethylguanosine</th>\n      <th>X - 12026</th>\n      <th>...</th>\n      <th>Complement component C9</th>\n      <th>Carbonic anhydrase 6</th>\n      <th>Kallistatin</th>\n      <th>Beta-2-microglobulin</th>\n      <th>C-reactive protein</th>\n      <th>Growth/differentiation factor 15</th>\n      <th>Alpha-(1,3)-fucosyltransferase 5</th>\n      <th>Trefoil factor 3</th>\n      <th>Troponin T</th>\n      <th>N-terminal pro-BNP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.020155</td>\n      <td>0.085807</td>\n      <td>-0.137053</td>\n      <td>0.060389</td>\n      <td>0.127048</td>\n      <td>-0.241752</td>\n      <td>0.358008</td>\n      <td>-0.128705</td>\n      <td>0.076749</td>\n      <td>0.010245</td>\n      <td>...</td>\n      <td>-0.038307</td>\n      <td>0.189493</td>\n      <td>0.034384</td>\n      <td>0.110211</td>\n      <td>-0.099851</td>\n      <td>0.001265</td>\n      <td>0.036117</td>\n      <td>0.006813</td>\n      <td>0.251452</td>\n      <td>0.160592</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.120475</td>\n      <td>-0.021728</td>\n      <td>-0.204108</td>\n      <td>0.014414</td>\n      <td>0.025543</td>\n      <td>-0.052912</td>\n      <td>0.124383</td>\n      <td>-0.269076</td>\n      <td>-0.075729</td>\n      <td>-0.105374</td>\n      <td>...</td>\n      <td>-0.026887</td>\n      <td>0.063893</td>\n      <td>0.245779</td>\n      <td>0.012016</td>\n      <td>-0.039549</td>\n      <td>-0.010779</td>\n      <td>0.036510</td>\n      <td>-0.003859</td>\n      <td>-0.337219</td>\n      <td>-0.002560</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.090287</td>\n      <td>0.013788</td>\n      <td>-0.059763</td>\n      <td>0.018466</td>\n      <td>0.003776</td>\n      <td>-0.017904</td>\n      <td>-0.069934</td>\n      <td>-0.051481</td>\n      <td>-0.193806</td>\n      <td>-0.042747</td>\n      <td>...</td>\n      <td>0.050901</td>\n      <td>-0.097281</td>\n      <td>0.093158</td>\n      <td>-0.043423</td>\n      <td>-0.112693</td>\n      <td>0.013895</td>\n      <td>0.095186</td>\n      <td>-0.008398</td>\n      <td>-0.388783</td>\n      <td>-0.112248</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.026253</td>\n      <td>0.038070</td>\n      <td>0.216268</td>\n      <td>0.016384</td>\n      <td>0.024989</td>\n      <td>0.021841</td>\n      <td>0.136472</td>\n      <td>-0.028681</td>\n      <td>-0.017122</td>\n      <td>-0.015498</td>\n      <td>...</td>\n      <td>0.048712</td>\n      <td>0.131387</td>\n      <td>0.028992</td>\n      <td>-0.087013</td>\n      <td>-0.209878</td>\n      <td>-0.108141</td>\n      <td>-0.190172</td>\n      <td>-0.018780</td>\n      <td>0.175538</td>\n      <td>-0.228993</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.038808</td>\n      <td>-0.044119</td>\n      <td>-0.123622</td>\n      <td>-0.030253</td>\n      <td>-0.018090</td>\n      <td>-0.023155</td>\n      <td>0.044109</td>\n      <td>-0.226136</td>\n      <td>-0.095078</td>\n      <td>-0.126534</td>\n      <td>...</td>\n      <td>-0.078458</td>\n      <td>-0.034569</td>\n      <td>0.054746</td>\n      <td>0.105940</td>\n      <td>0.152730</td>\n      <td>0.101574</td>\n      <td>-0.109237</td>\n      <td>0.018332</td>\n      <td>0.022042</td>\n      <td>0.265967</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>899</th>\n      <td>-0.087872</td>\n      <td>0.001957</td>\n      <td>0.302890</td>\n      <td>-0.003579</td>\n      <td>-0.030582</td>\n      <td>0.018038</td>\n      <td>-0.275302</td>\n      <td>0.193258</td>\n      <td>0.000543</td>\n      <td>-0.050731</td>\n      <td>...</td>\n      <td>0.228754</td>\n      <td>0.040530</td>\n      <td>-0.230100</td>\n      <td>0.037203</td>\n      <td>0.589362</td>\n      <td>-0.047364</td>\n      <td>0.170722</td>\n      <td>-0.008223</td>\n      <td>-0.064778</td>\n      <td>0.097253</td>\n    </tr>\n    <tr>\n      <th>900</th>\n      <td>0.022156</td>\n      <td>-0.018443</td>\n      <td>-0.002874</td>\n      <td>-0.057762</td>\n      <td>-0.073527</td>\n      <td>-0.149700</td>\n      <td>-0.162992</td>\n      <td>-0.004981</td>\n      <td>-0.082313</td>\n      <td>-0.130706</td>\n      <td>...</td>\n      <td>-0.034999</td>\n      <td>0.052366</td>\n      <td>0.138812</td>\n      <td>-0.040160</td>\n      <td>-0.219306</td>\n      <td>-0.013538</td>\n      <td>-0.090685</td>\n      <td>-0.022949</td>\n      <td>-0.290100</td>\n      <td>0.172107</td>\n    </tr>\n    <tr>\n      <th>901</th>\n      <td>0.010479</td>\n      <td>-0.052656</td>\n      <td>-0.165958</td>\n      <td>-0.004135</td>\n      <td>-0.007765</td>\n      <td>-0.225824</td>\n      <td>0.113716</td>\n      <td>-0.121771</td>\n      <td>0.007362</td>\n      <td>-0.041303</td>\n      <td>...</td>\n      <td>0.020628</td>\n      <td>-0.052411</td>\n      <td>0.053268</td>\n      <td>-0.137826</td>\n      <td>-0.039688</td>\n      <td>-0.096756</td>\n      <td>-0.030811</td>\n      <td>-0.026811</td>\n      <td>-0.155213</td>\n      <td>-0.193125</td>\n    </tr>\n    <tr>\n      <th>902</th>\n      <td>0.231681</td>\n      <td>-0.008008</td>\n      <td>0.079493</td>\n      <td>-0.021840</td>\n      <td>-0.033465</td>\n      <td>-0.303659</td>\n      <td>-0.048747</td>\n      <td>0.145802</td>\n      <td>-0.066817</td>\n      <td>-0.070219</td>\n      <td>...</td>\n      <td>-0.034996</td>\n      <td>0.360527</td>\n      <td>-0.003323</td>\n      <td>-0.070229</td>\n      <td>-0.202823</td>\n      <td>-0.101221</td>\n      <td>0.053024</td>\n      <td>-0.026247</td>\n      <td>-0.258822</td>\n      <td>-0.120955</td>\n    </tr>\n    <tr>\n      <th>903</th>\n      <td>-0.252429</td>\n      <td>0.052078</td>\n      <td>-0.057335</td>\n      <td>-0.005953</td>\n      <td>-0.028814</td>\n      <td>-0.060162</td>\n      <td>-0.120434</td>\n      <td>0.010339</td>\n      <td>-0.026376</td>\n      <td>-0.012517</td>\n      <td>...</td>\n      <td>0.002016</td>\n      <td>-0.323179</td>\n      <td>-0.014994</td>\n      <td>-0.041288</td>\n      <td>0.065322</td>\n      <td>0.043078</td>\n      <td>0.180451</td>\n      <td>0.005509</td>\n      <td>0.234726</td>\n      <td>-0.177583</td>\n    </tr>\n  </tbody>\n</table>\n<p>904 rows × 27 columns</p>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrate_final_embeddings()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:11:21.955579814Z",
     "start_time": "2024-02-09T05:11:21.910437825Z"
    }
   },
   "id": "38e6e0555f233b8f",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        (N(1) + N(8))-acetylspermidine  5-acetylamino-6-amino-3-methyluracil  \\\nsid                                                                            \n10010J                       -0.190253                              2.198677   \n10031R                        1.137203                             -0.556750   \n10032T                       -0.852246                              0.353310   \n10052Z                        0.247807                              0.975486   \n10055F                       -0.366317                             -1.130497   \n...                                ...                                   ...   \n25551W                        0.829446                              0.050146   \n25563D                       -0.209134                             -0.472576   \n25564F                       -0.098917                             -1.349235   \n25571C                       -2.186909                             -0.205182   \n25581F                        2.382749                              1.334424   \n\n        5-hydroxyhexanoate  adrenate (22:4n6)  C-glycosyltryptophan  \\\nsid                                                                   \n10010J            0.951882           1.728713              1.696511   \n10031R            1.417606           0.412620              0.341081   \n10032T            0.415074           0.528622              0.050416   \n10052Z           -1.502064           0.469000              0.333686   \n10055F            0.858598          -0.866032             -0.241559   \n...                    ...                ...                   ...   \n25551W           -2.103680          -0.102467             -0.408373   \n25563D            0.019962          -1.653494             -0.981828   \n25564F            1.152642          -0.118369             -0.103682   \n25571C           -0.552108          -0.625187             -0.446872   \n25581F            0.398210          -0.170399             -0.384768   \n\n        phosphocholine  ergothioneine  myristoleoylcarnitine (C14:1)*  \\\nsid                                                                     \n10010J        1.810291      -1.921257                        0.875180   \n10031R        0.396214      -0.667501                        1.829685   \n10032T        0.134069       0.375303                        0.350066   \n10052Z       -0.163554      -0.732381                        0.195025   \n10055F        0.173388      -0.236713                        1.537696   \n...                ...            ...                             ...   \n25551W       -0.135074       1.477414                       -1.314134   \n25563D        1.120985       0.874699                        0.033871   \n25564F        1.691015      -0.610257                        0.828027   \n25571C        2.273861       0.261602                       -0.991433   \n25581F        0.450509       0.646311                       -0.070302   \n\n        N2,N2-dimethylguanosine  X - 12026  ...  Complement component C9  \\\nsid                                         ...                            \n10010J                 0.892684   0.124223  ...                 0.322382   \n10031R                -0.880821  -1.277666  ...                 0.226270   \n10032T                -2.254200  -0.518312  ...                -0.428367   \n10052Z                -0.199153  -0.187908  ...                -0.409941   \n10055F                -1.105873  -1.534229  ...                 0.660275   \n...                         ...        ...  ...                      ...   \n25551W                 0.006312  -0.615117  ...                -1.925122   \n25563D                -0.957397  -1.584809  ...                 0.294543   \n25564F                 0.085634  -0.500804  ...                -0.173596   \n25571C                -0.777158  -0.851403  ...                 0.294511   \n25581F                -0.306786  -0.151771  ...                -0.016965   \n\n        Carbonic anhydrase 6  Kallistatin  Beta-2-microglobulin  \\\nsid                                                               \n10010J             -1.118447    -0.234189              0.926362   \n10031R             -0.377116    -1.673974              0.100996   \n10032T              0.574186    -0.634489             -0.364988   \n10052Z             -0.775491    -0.197459             -0.731378   \n10055F              0.204040    -0.372867              0.890467   \n...                      ...          ...                   ...   \n25551W             -0.239220     1.567190              0.312708   \n25563D             -0.309081    -0.945435             -0.337557   \n25564F              0.309346    -0.362805             -1.158474   \n25571C             -2.127945     0.022630             -0.590303   \n25581F              1.907504     0.102125             -0.347039   \n\n        C-reactive protein  Growth/differentiation factor 15  \\\nsid                                                            \n10010J            0.557443                          0.017068   \n10031R            0.220795                         -0.145500   \n10032T            0.629139                          0.187559   \n10052Z            1.171704                         -1.459689   \n10055F           -0.852658                          1.371039   \n...                    ...                               ...   \n25551W           -3.290274                         -0.639323   \n25563D            1.224334                         -0.182733   \n25564F            0.221569                         -1.306005   \n25571C            1.132313                         -1.366281   \n25581F           -0.364680                          0.581469   \n\n        Alpha-(1,3)-fucosyltransferase 5  Trefoil factor 3  Troponin T  \\\nsid                                                                      \n10010J                         -0.382598          0.234377    0.593199   \n10031R                         -0.386756         -0.132752   -0.795531   \n10032T                         -1.008337         -0.288907   -0.917175   \n10052Z                          2.014551         -0.646074    0.414110   \n10055F                          1.157176          0.630683    0.051998   \n...                                  ...               ...         ...   \n25551W                         -1.808501         -0.282904   -0.152816   \n25563D                          0.960652         -0.789511   -0.684374   \n25564F                          0.326384         -0.922360   -0.366161   \n25571C                         -0.561698         -0.902983   -0.610586   \n25581F                         -1.911572          0.189512    0.553741   \n\n        N-terminal pro-BNP  \nsid                         \n10010J            1.282038  \n10031R           -0.020434  \n10032T           -0.896100  \n10052Z           -1.828103  \n10055F            2.123272  \n...                    ...  \n25551W            0.776388  \n25563D            1.373968  \n25564F           -1.541756  \n25571C           -0.965605  \n25581F           -1.417684  \n\n[904 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>(N(1) + N(8))-acetylspermidine</th>\n      <th>5-acetylamino-6-amino-3-methyluracil</th>\n      <th>5-hydroxyhexanoate</th>\n      <th>adrenate (22:4n6)</th>\n      <th>C-glycosyltryptophan</th>\n      <th>phosphocholine</th>\n      <th>ergothioneine</th>\n      <th>myristoleoylcarnitine (C14:1)*</th>\n      <th>N2,N2-dimethylguanosine</th>\n      <th>X - 12026</th>\n      <th>...</th>\n      <th>Complement component C9</th>\n      <th>Carbonic anhydrase 6</th>\n      <th>Kallistatin</th>\n      <th>Beta-2-microglobulin</th>\n      <th>C-reactive protein</th>\n      <th>Growth/differentiation factor 15</th>\n      <th>Alpha-(1,3)-fucosyltransferase 5</th>\n      <th>Trefoil factor 3</th>\n      <th>Troponin T</th>\n      <th>N-terminal pro-BNP</th>\n    </tr>\n    <tr>\n      <th>sid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10010J</th>\n      <td>-0.190253</td>\n      <td>2.198677</td>\n      <td>0.951882</td>\n      <td>1.728713</td>\n      <td>1.696511</td>\n      <td>1.810291</td>\n      <td>-1.921257</td>\n      <td>0.875180</td>\n      <td>0.892684</td>\n      <td>0.124223</td>\n      <td>...</td>\n      <td>0.322382</td>\n      <td>-1.118447</td>\n      <td>-0.234189</td>\n      <td>0.926362</td>\n      <td>0.557443</td>\n      <td>0.017068</td>\n      <td>-0.382598</td>\n      <td>0.234377</td>\n      <td>0.593199</td>\n      <td>1.282038</td>\n    </tr>\n    <tr>\n      <th>10031R</th>\n      <td>1.137203</td>\n      <td>-0.556750</td>\n      <td>1.417606</td>\n      <td>0.412620</td>\n      <td>0.341081</td>\n      <td>0.396214</td>\n      <td>-0.667501</td>\n      <td>1.829685</td>\n      <td>-0.880821</td>\n      <td>-1.277666</td>\n      <td>...</td>\n      <td>0.226270</td>\n      <td>-0.377116</td>\n      <td>-1.673974</td>\n      <td>0.100996</td>\n      <td>0.220795</td>\n      <td>-0.145500</td>\n      <td>-0.386756</td>\n      <td>-0.132752</td>\n      <td>-0.795531</td>\n      <td>-0.020434</td>\n    </tr>\n    <tr>\n      <th>10032T</th>\n      <td>-0.852246</td>\n      <td>0.353310</td>\n      <td>0.415074</td>\n      <td>0.528622</td>\n      <td>0.050416</td>\n      <td>0.134069</td>\n      <td>0.375303</td>\n      <td>0.350066</td>\n      <td>-2.254200</td>\n      <td>-0.518312</td>\n      <td>...</td>\n      <td>-0.428367</td>\n      <td>0.574186</td>\n      <td>-0.634489</td>\n      <td>-0.364988</td>\n      <td>0.629139</td>\n      <td>0.187559</td>\n      <td>-1.008337</td>\n      <td>-0.288907</td>\n      <td>-0.917175</td>\n      <td>-0.896100</td>\n    </tr>\n    <tr>\n      <th>10052Z</th>\n      <td>0.247807</td>\n      <td>0.975486</td>\n      <td>-1.502064</td>\n      <td>0.469000</td>\n      <td>0.333686</td>\n      <td>-0.163554</td>\n      <td>-0.732381</td>\n      <td>0.195025</td>\n      <td>-0.199153</td>\n      <td>-0.187908</td>\n      <td>...</td>\n      <td>-0.409941</td>\n      <td>-0.775491</td>\n      <td>-0.197459</td>\n      <td>-0.731378</td>\n      <td>1.171704</td>\n      <td>-1.459689</td>\n      <td>2.014551</td>\n      <td>-0.646074</td>\n      <td>0.414110</td>\n      <td>-1.828103</td>\n    </tr>\n    <tr>\n      <th>10055F</th>\n      <td>-0.366317</td>\n      <td>-1.130497</td>\n      <td>0.858598</td>\n      <td>-0.866032</td>\n      <td>-0.241559</td>\n      <td>0.173388</td>\n      <td>-0.236713</td>\n      <td>1.537696</td>\n      <td>-1.105873</td>\n      <td>-1.534229</td>\n      <td>...</td>\n      <td>0.660275</td>\n      <td>0.204040</td>\n      <td>-0.372867</td>\n      <td>0.890467</td>\n      <td>-0.852658</td>\n      <td>1.371039</td>\n      <td>1.157176</td>\n      <td>0.630683</td>\n      <td>0.051998</td>\n      <td>2.123272</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25551W</th>\n      <td>0.829446</td>\n      <td>0.050146</td>\n      <td>-2.103680</td>\n      <td>-0.102467</td>\n      <td>-0.408373</td>\n      <td>-0.135074</td>\n      <td>1.477414</td>\n      <td>-1.314134</td>\n      <td>0.006312</td>\n      <td>-0.615117</td>\n      <td>...</td>\n      <td>-1.925122</td>\n      <td>-0.239220</td>\n      <td>1.567190</td>\n      <td>0.312708</td>\n      <td>-3.290274</td>\n      <td>-0.639323</td>\n      <td>-1.808501</td>\n      <td>-0.282904</td>\n      <td>-0.152816</td>\n      <td>0.776388</td>\n    </tr>\n    <tr>\n      <th>25563D</th>\n      <td>-0.209134</td>\n      <td>-0.472576</td>\n      <td>0.019962</td>\n      <td>-1.653494</td>\n      <td>-0.981828</td>\n      <td>1.120985</td>\n      <td>0.874699</td>\n      <td>0.033871</td>\n      <td>-0.957397</td>\n      <td>-1.584809</td>\n      <td>...</td>\n      <td>0.294543</td>\n      <td>-0.309081</td>\n      <td>-0.945435</td>\n      <td>-0.337557</td>\n      <td>1.224334</td>\n      <td>-0.182733</td>\n      <td>0.960652</td>\n      <td>-0.789511</td>\n      <td>-0.684374</td>\n      <td>1.373968</td>\n    </tr>\n    <tr>\n      <th>25564F</th>\n      <td>-0.098917</td>\n      <td>-1.349235</td>\n      <td>1.152642</td>\n      <td>-0.118369</td>\n      <td>-0.103682</td>\n      <td>1.691015</td>\n      <td>-0.610257</td>\n      <td>0.828027</td>\n      <td>0.085634</td>\n      <td>-0.500804</td>\n      <td>...</td>\n      <td>-0.173596</td>\n      <td>0.309346</td>\n      <td>-0.362805</td>\n      <td>-1.158474</td>\n      <td>0.221569</td>\n      <td>-1.306005</td>\n      <td>0.326384</td>\n      <td>-0.922360</td>\n      <td>-0.366161</td>\n      <td>-1.541756</td>\n    </tr>\n    <tr>\n      <th>25571C</th>\n      <td>-2.186909</td>\n      <td>-0.205182</td>\n      <td>-0.552108</td>\n      <td>-0.625187</td>\n      <td>-0.446872</td>\n      <td>2.273861</td>\n      <td>0.261602</td>\n      <td>-0.991433</td>\n      <td>-0.777158</td>\n      <td>-0.851403</td>\n      <td>...</td>\n      <td>0.294511</td>\n      <td>-2.127945</td>\n      <td>0.022630</td>\n      <td>-0.590303</td>\n      <td>1.132313</td>\n      <td>-1.366281</td>\n      <td>-0.561698</td>\n      <td>-0.902983</td>\n      <td>-0.610586</td>\n      <td>-0.965605</td>\n    </tr>\n    <tr>\n      <th>25581F</th>\n      <td>2.382749</td>\n      <td>1.334424</td>\n      <td>0.398210</td>\n      <td>-0.170399</td>\n      <td>-0.384768</td>\n      <td>0.450509</td>\n      <td>0.646311</td>\n      <td>-0.070302</td>\n      <td>-0.306786</td>\n      <td>-0.151771</td>\n      <td>...</td>\n      <td>-0.016965</td>\n      <td>1.907504</td>\n      <td>0.102125</td>\n      <td>-0.347039</td>\n      <td>-0.364680</td>\n      <td>0.581469</td>\n      <td>-1.911572</td>\n      <td>0.189512</td>\n      <td>0.553741</td>\n      <td>-1.417684</td>\n    </tr>\n  </tbody>\n</table>\n<p>904 rows × 27 columns</p>\n</div>"
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset_sid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T18:04:36.472032326Z",
     "start_time": "2024-02-08T18:04:36.452404454Z"
    }
   },
   "id": "d2b7227477a5b894",
   "execution_count": 235
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, hidden_dim_name, layer_num=2, dropout=True, **kwargs):\n",
    "        super(GCN, self).__init__()\n",
    "        self.hidden_dim_name = hidden_dim_name\n",
    "        self.layer_num = layer_num\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.conv_first = tg.nn.GCNConv(input_dim, hidden_dim)\n",
    "        self.conv_hidden = nn.ModuleList([tg.nn.GCNConv(hidden_dim, hidden_dim) for i in range(layer_num - 2)])\n",
    "        # self.conv_out = tg.nn.GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        import pandas as pd\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        # print(\"X Before the 1st Conv %s\" % x)\n",
    "\n",
    "        x = self.conv_first(x, edge_index, edge_weight)\n",
    "\n",
    "        # print(\"X After the 1st Conv %s\" % x)\n",
    "\n",
    "        t_np = x.cpu().data.numpy()  # convert to Numpy array\n",
    "        df = pd.DataFrame(t_np)  # convert to a dataframe\n",
    "        df.to_csv(\"HiddenLayerOutput_GCN_%s\" % self.hidden_dim_name, index=False)  # save to file\n",
    "\n",
    "        # print(\"Data after the first convolution %s\" % t_np)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        # x = F.tanh(x)\n",
    "\n",
    "        # print(\"X After RELU %s\" % x)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = F.dropout(x, training=self.training)\n",
    "\n",
    "        # print(\"X After Dropout %s\" % x)\n",
    "\n",
    "        for i in range(self.layer_num-2):\n",
    "            x = self.conv_hidden[i](x.clone(), edge_index, edge_weight)\n",
    "\n",
    "            # print(\"X After %d Conv %s\" % (i, x))\n",
    "\n",
    "            t_np = x.cpu().data.numpy()  # convert to Numpy array\n",
    "            df = pd.DataFrame(t_np)  # convert to a dataframe\n",
    "            df.to_csv(\"HiddenLayerOutput_GCN_%s\" % self.hidden_dim_name, index=False)  # save to file\n",
    "\n",
    "            # print(\"Data after inner convolution %s\" % t_np)\n",
    "            x = F.relu(x)\n",
    "            # x = F.tanh(x)\n",
    "            if self.dropout:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "\n",
    "        # x = self.conv_out(x, edge_index, edge_weight)\n",
    "        # print(\"X After Conv Out %s\" % x)\n",
    "\n",
    "        # print(\"Data after the last convolution %s\" % x.cpu().data.numpy())\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:55:40.359171708Z",
     "start_time": "2024-02-09T05:55:40.351206866Z"
    }
   },
   "id": "daa1bb5ed6fabe58",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "class DownstreamTaskModelNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(DownstreamTaskModelNN, self).__init__()\n",
    "        # Learnable Scaling Factors\n",
    "        # self.scalars = nn.Parameter(torch.ones(input_size))\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1) # TODO: What other Layer are there!\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # self.tanh = nn.Tanh()\n",
    "        # self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        # self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "        # self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size1, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x * self.scalars  # Apply the Learned Scalars to the Input \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        # x = self.tanh(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        # print(\"Scalars Values %s\" % self.scalars.data.numpy())\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T05:40:56.703397470Z",
     "start_time": "2024-02-09T05:40:56.698667648Z"
    }
   },
   "id": "948bc12b418046c1",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([27, 16]) and output[0] has a shape of torch.Size([]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 88\u001B[0m\n\u001B[1;32m     86\u001B[0m     gnn_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     87\u001B[0m     out\u001B[38;5;241m.\u001B[39mgrad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;66;03m# Reset gradients\u001B[39;00m\n\u001B[0;32m---> 88\u001B[0m     prediction_loss\u001B[38;5;241m.\u001B[39mbackward(torch\u001B[38;5;241m.\u001B[39mones_like(out), retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# gnn_loss = gnn_criterion(outputs, labels)\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;66;03m# print(\u001B[39;00m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;66;03m#     f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx + 1}, GNN Loss: {gnn_loss.item():.4f}, Phenotype Loss: {prediction_loss.item():.4f}\")\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# torch.nn.utils.clip_grad_norm_(gnn_model.parameters(), 0.01)\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;66;03m# torch.nn.utils.clip_grad_norm_(downstream_model.parameters(), 0.01)\u001B[39;00m\n\u001B[1;32m    101\u001B[0m     prediction_optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/anaconda3/envs/GNNs/lib/python3.11/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    493\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    494\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/GNNs/lib/python3.11/site-packages/torch/autograd/__init__.py:244\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    235\u001B[0m inputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    236\u001B[0m     (inputs,)\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, torch\u001B[38;5;241m.\u001B[39mTensor)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m()\n\u001B[1;32m    241\u001B[0m )\n\u001B[1;32m    243\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[0;32m--> 244\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n",
      "File \u001B[0;32m~/anaconda3/envs/GNNs/lib/python3.11/site-packages/torch/autograd/__init__.py:88\u001B[0m, in \u001B[0;36m_make_grads\u001B[0;34m(outputs, grads, is_grads_batched)\u001B[0m\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     71\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     72\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdimension of each grad_output as the batch dimension. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     85\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatched, consider using vmap.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     86\u001B[0m         )\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 88\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     89\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMismatch in shape: grad_output[\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     90\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(grads\u001B[38;5;241m.\u001B[39mindex(grad))\n\u001B[1;32m     91\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m] has a shape of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     92\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(grad_shape)\n\u001B[1;32m     93\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and output[\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     94\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(outputs\u001B[38;5;241m.\u001B[39mindex(out))\n\u001B[1;32m     95\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m] has a shape of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     96\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(out_shape)\n\u001B[1;32m     97\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     98\u001B[0m         )\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mis_complex \u001B[38;5;241m!=\u001B[39m grad\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mis_complex:\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor complex Tensors, both grad_output and output\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    102\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m are required to have the same dtype.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    111\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    112\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([27, 16]) and output[0] has a shape of torch.Size([])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Initialize GNNs\n",
    "input_dim = dataset.x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim = 16\n",
    "layer_num = 3\n",
    "dropout = 0.5\n",
    "lr = 0.002244\n",
    "weight_decay = 0.034040\n",
    "\n",
    "gnn_model = GCN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, layer_num=layer_num, dropout=dropout, hidden_dim_name=dataset_name)\n",
    "gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "gnn_criterion = torch.nn.CrossEntropyLoss() # torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "X = original_dataset_sid\n",
    "y = complete_original_dataset['finalgold_visit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # TODO: Maybe better if you update the integration function to take these as parameters and update them \n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 128\n",
    "output_size = 3  # 3 Classes\n",
    "# Initialize the Downstream Task Model\n",
    "downstream_model = DownstreamTaskModelNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "prediction_optimizer = optim.Adam(downstream_model.parameters(), lr=0.001631, weight_decay=0.030408)\n",
    "prediction_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# wandb.init(project='debuggingnn')\n",
    "# wandb.watch(gnn_model, gnn_criterion, log='all')\n",
    "# wandb.watch(downstream_model, prediction_criterion, log='all')\n",
    "\n",
    "num_epochs = 16\n",
    "for epoch in range(num_epochs):\n",
    "    gnn_optimizer.zero_grad()\n",
    "    # Forward Pass through GNN Layers\n",
    "    gnn_model.train()  # TODO: Do we Need this Training?\n",
    "    out = gnn_model(data)# .flatten()\n",
    "    # print(\"Shape of hidden space\")\n",
    "    # print(out.shape)\n",
    "\n",
    "    # Integrate embeddings in the original dataset\n",
    "    # TODO: Refactor this Code and Consider a Better Way to Pick a Dimension, should we do backpropagate here to get better prediction loss\n",
    "    dim_losses = []\n",
    "    X_sliced = integrate_final_embeddings()\n",
    "\n",
    "    # for i in range(hidden_dim): # TODO: Consider processing dimensions and batches together; also run multiple threads for each dimension and do the optimization for\n",
    "    #     print(\"***** Processing Dimension %d *****\" % i)\n",
    "    # all dimensions\n",
    "    # original_dataset_tmp = pd.DataFrame(X_sliced[8])\n",
    "    # Incorporate Embeddings into the Original Dataset\n",
    "    original_dataset_tmp = X_sliced\n",
    "    X_train, X_test, y_train, y_test = train_test_split(original_dataset_tmp, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch Tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "    y_train_tensor = torch.LongTensor(y_train.to_numpy())\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "    y_test_tensor = torch.LongTensor(y_test.to_numpy())\n",
    "\n",
    "    # Create DataLoader for Training\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        prediction_optimizer.zero_grad()\n",
    "        outputs = downstream_model(inputs)\n",
    "        prediction_loss = prediction_criterion(outputs, labels)\n",
    "        prediction_loss.backward(retain_graph=True)\n",
    "\n",
    "        \n",
    "\n",
    "        gnn_optimizer.zero_grad()\n",
    "        out.grad = None # Reset gradients\n",
    "        out.backward(torch.ones_like(out), retain_graph=True)\n",
    "    # gnn_loss = gnn_criterion(outputs, labels)\n",
    "    # print(\n",
    "    #     f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx + 1}, GNN Loss: {gnn_loss.item():.4f}, Phenotype Loss: {prediction_loss.item():.4f}\")\n",
    "\n",
    "    # prediction_loss.backward(retain_graph=True)\n",
    "    # gnn_loss.backward()\n",
    "\n",
    "    # Gradient clipping\n",
    "    # torch.nn.utils.clip_grad_norm_(gnn_model.parameters(), 0.01)\n",
    "    # torch.nn.utils.clip_grad_norm_(downstream_model.parameters(), 0.01)\n",
    "\n",
    "\n",
    "        prediction_optimizer.step()\n",
    "        gnn_optimizer.step()\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx + 1}, Phenotype Loss: {prediction_loss.item():.4f}\")\n",
    "        # prediction_optimizer.step()\n",
    "    \n",
    "        print(\"Gradients for the GNN Model!!!! \")\n",
    "        # Track gradients (gnn_model) and model parameters in TensorBoard\n",
    "        for name, param in gnn_model.named_parameters():\n",
    "            print(f'{name}.grad', param.grad, epoch)\n",
    "            writer.add_histogram(name, param, epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f'{name}.grad', param.grad, epoch)\n",
    "    \n",
    "print(\"Training finished.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T03:15:12.141420900Z",
     "start_time": "2024-02-13T03:15:11.045421342Z"
    }
   },
   "id": "1c89ac34011710ad",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5367647058823529\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.91      0.72       132\n",
      "           1       0.37      0.29      0.33        90\n",
      "           2       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.54       272\n",
      "   macro avg       0.32      0.40      0.35       272\n",
      "weighted avg       0.41      0.54      0.46       272\n",
      "\n",
      "ROC AUC Score: 0.7003393769264221\n",
      "Confusion Matrix: [[120  12   0]\n",
      " [ 64  26   0]\n",
      " [ 18  32   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shussein/anaconda3/envs/GNNs/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/shussein/anaconda3/envs/GNNs/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/shussein/anaconda3/envs/GNNs/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchmetrics.classification import MulticlassAUROC\n",
    "\n",
    "\n",
    "# Evaluate the Model on the Test Dataset\n",
    "with torch.no_grad():\n",
    "    gnn_model.eval()\n",
    "    out = gnn_model(data)# .flatten()\n",
    "    downstream_model.eval()\n",
    "    \n",
    "X_sliced = integrate_final_embeddings()\n",
    "\n",
    "# for i in range(hidden_dim):\n",
    "original_dataset_tmp = X_sliced\n",
    "# Incorporate Embeddings into the Original Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(original_dataset_tmp, y, test_size=0.30, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train.to_numpy())\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.LongTensor(y_test.to_numpy())\n",
    "    \n",
    "with torch.no_grad():\n",
    "    outputs = downstream_model(X_test_tensor)\n",
    "    l, predicted = torch.max(outputs, 1)\n",
    "    accuracy = torch.sum(predicted == y_test_tensor).item() / len(y_test)\n",
    "    # auc_score = roc_auc_score(l, predicted, multi_class='ovr')\n",
    "    confusion_mtrx = confusion_matrix(y_test_tensor, predicted)\n",
    "    mc_auroc = MulticlassAUROC(num_classes=3, average='macro', thresholds=None)\n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "    print(f'Classification Report: {classification_report(y_test_tensor, predicted)}')\n",
    "    print(f'ROC AUC Score: {mc_auroc(outputs, y_test_tensor)}')\n",
    "    print(f'Confusion Matrix: {confusion_mtrx}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T02:51:11.355534291Z",
     "start_time": "2024-02-13T02:51:10.157591097Z"
    }
   },
   "id": "74c255113e21f73d",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.5753e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,\n          1.5555e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00, -5.2061e-01, -2.4834e-01, -8.1043e-02,  0.0000e+00,\n         -0.0000e+00],\n        [ 3.2311e-01,  0.0000e+00,  1.7915e-01, -2.6798e-01, -0.0000e+00,\n          1.9596e-01,  0.0000e+00, -0.0000e+00,  2.3600e-01,  0.0000e+00,\n          0.0000e+00, -5.5491e-01, -0.0000e+00,  1.6399e-02,  0.0000e+00,\n          0.0000e+00],\n        [ 3.3281e-01,  0.0000e+00,  1.8026e-01, -0.0000e+00, -0.0000e+00,\n          0.0000e+00,  5.2842e-02, -1.6581e-02,  2.4586e-01,  0.0000e+00,\n          0.0000e+00, -4.6927e-01, -1.3565e-01, -6.8181e-02,  4.9792e-02,\n         -0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  1.9285e-01, -0.0000e+00, -1.9400e-01,\n          0.0000e+00,  0.0000e+00, -0.0000e+00,  2.6600e-01,  0.0000e+00,\n          0.0000e+00, -6.0589e-01, -0.0000e+00,  0.0000e+00,  1.2007e-01,\n          0.0000e+00],\n        [ 3.7736e-01,  1.5256e-01,  1.8911e-01, -2.7995e-01, -0.0000e+00,\n          2.3223e-01,  1.8404e-01, -0.0000e+00,  2.4778e-01, -5.5521e-04,\n          0.0000e+00, -6.0522e-01, -0.0000e+00,  3.6024e-02,  0.0000e+00,\n          5.3655e-02],\n        [ 3.1288e-01,  0.0000e+00,  1.7924e-01, -0.0000e+00, -1.7174e-01,\n          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4097e-01,\n          0.0000e+00, -0.0000e+00, -2.0287e-01, -8.8423e-02,  0.0000e+00,\n         -0.0000e+00],\n        [ 3.3942e-01,  1.7791e-01,  0.0000e+00, -2.2161e-01, -5.5288e-02,\n          0.0000e+00,  0.0000e+00, -0.0000e+00,  3.0347e-01,  1.2731e-01,\n          3.6465e-01, -0.0000e+00, -0.0000e+00, -1.0487e-01,  0.0000e+00,\n         -0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  1.9743e-01, -2.2846e-01, -1.0292e-01,\n          1.9906e-01,  0.0000e+00, -0.0000e+00,  0.0000e+00,  6.6691e-02,\n          0.0000e+00, -0.0000e+00, -1.2375e-01, -5.3383e-02,  1.0182e-02,\n         -0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.6928e-01, -0.0000e+00,\n          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n          0.0000e+00, -6.2424e-01, -2.0936e-01,  5.0392e-02,  0.0000e+00,\n          0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  2.1346e-01, -2.3686e-01, -1.4968e-01,\n          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n          0.0000e+00, -0.0000e+00, -1.6083e-01,  6.7908e-02,  6.4651e-02,\n          5.3453e-02],\n        [ 0.0000e+00,  0.0000e+00,  1.7738e-01, -2.0707e-01, -1.7162e-01,\n          2.3171e-01,  0.0000e+00, -1.1226e-01,  0.0000e+00, -0.0000e+00,\n          4.7773e-01, -0.0000e+00, -1.8663e-01,  6.5399e-02,  0.0000e+00,\n          0.0000e+00],\n        [ 0.0000e+00,  1.6450e-01,  0.0000e+00, -0.0000e+00, -8.5778e-02,\n          1.2209e-01,  0.0000e+00, -8.8581e-03,  2.2534e-01,  7.6573e-02,\n          3.2777e-01, -0.0000e+00, -1.0953e-01, -6.7004e-02,  6.1969e-02,\n         -0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  1.6911e-01, -0.0000e+00, -2.4689e-01,\n          2.6880e-01,  2.2474e-01, -5.7133e-02,  0.0000e+00,  1.1075e-03,\n          4.7654e-01, -0.0000e+00, -2.5168e-01,  6.4249e-02,  1.6444e-01,\n          0.0000e+00],\n        [ 3.1092e-01,  0.0000e+00,  0.0000e+00, -2.5184e-01, -1.9610e-01,\n          2.1887e-01,  2.1083e-01, -5.5711e-02,  0.0000e+00, -0.0000e+00,\n          4.1694e-01, -5.6126e-01, -2.1004e-01,  5.5433e-02,  1.3877e-01,\n          9.2959e-02],\n        [ 0.0000e+00,  3.4130e-01,  0.0000e+00, -0.0000e+00, -3.9956e-01,\n          0.0000e+00,  0.0000e+00, -8.4044e-02,  0.0000e+00,  0.0000e+00,\n          0.0000e+00, -0.0000e+00, -4.3741e-01, -0.0000e+00,  0.0000e+00,\n          2.5529e-02],\n        [ 0.0000e+00,  0.0000e+00,  1.3352e-01, -3.1704e-01, -1.5419e-01,\n          1.3588e-01,  0.0000e+00,  0.0000e+00,  1.9834e-01,  1.0044e-01,\n          0.0000e+00, -4.6411e-01, -0.0000e+00, -0.0000e+00,  1.3305e-01,\n          0.0000e+00],\n        [ 2.8761e-01,  1.5527e-01,  1.6092e-01, -2.6361e-01, -0.0000e+00,\n          0.0000e+00,  1.4602e-01, -3.1598e-02,  0.0000e+00,  4.0287e-02,\n          3.8274e-01, -5.1437e-01, -1.6977e-01, -0.0000e+00,  1.2749e-01,\n         -1.0118e-02],\n        [ 0.0000e+00,  1.5280e-01,  0.0000e+00, -2.3527e-01, -0.0000e+00,\n          1.5941e-01,  0.0000e+00, -0.0000e+00,  2.4625e-01,  5.4521e-02,\n          3.6362e-01, -4.8810e-01, -1.7326e-01, -6.7615e-02,  6.4697e-02,\n         -9.0511e-02],\n        [ 0.0000e+00,  2.0354e-01,  1.4770e-01, -0.0000e+00, -1.4978e-01,\n          8.4310e-02,  0.0000e+00,  0.0000e+00,  2.5603e-01,  1.4584e-01,\n          0.0000e+00, -4.6379e-01, -0.0000e+00, -1.3707e-01,  1.4468e-01,\n         -0.0000e+00],\n        [ 2.8886e-01,  0.0000e+00,  1.7052e-01, -2.4242e-01, -9.9487e-02,\n          1.1365e-01,  5.4419e-02, -0.0000e+00,  2.2969e-01,  0.0000e+00,\n          0.0000e+00, -4.4229e-01, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n         -1.1517e-01],\n        [ 3.6870e-01,  0.0000e+00,  1.6467e-01, -2.9535e-01, -2.5129e-01,\n          2.7336e-01,  0.0000e+00, -6.2688e-02,  0.0000e+00, -3.1706e-03,\n          4.7223e-01, -0.0000e+00, -2.5434e-01,  7.0787e-02,  1.6355e-01,\n          1.0697e-01],\n        [ 1.5929e-01,  1.6608e-01,  1.3759e-01, -2.2622e-01, -2.8797e-02,\n         -0.0000e+00,  6.6450e-02,  3.4253e-02,  2.0141e-01,  9.4884e-02,\n          2.4556e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00,  7.7860e-02,\n         -0.0000e+00],\n        [ 3.4828e-01,  1.3235e-01,  1.3679e-01, -3.0679e-01, -0.0000e+00,\n          0.0000e+00,  0.0000e+00, -8.6684e-02,  0.0000e+00,  2.0440e-02,\n          4.5473e-01, -0.0000e+00, -3.1960e-01,  3.3983e-02,  1.8352e-01,\n          0.0000e+00],\n        [ 3.0296e-01,  0.0000e+00,  1.5310e-01, -2.6356e-01, -1.6439e-01,\n          0.0000e+00,  0.0000e+00, -2.8746e-02,  2.4034e-01,  8.5845e-02,\n          0.0000e+00, -0.0000e+00, -0.0000e+00, -4.4999e-02,  0.0000e+00,\n         -3.5624e-02],\n        [ 3.6121e-01,  0.0000e+00,  1.7583e-01, -0.0000e+00, -1.9449e-01,\n          2.4574e-01,  1.7915e-01, -0.0000e+00,  2.5870e-01,  0.0000e+00,\n          0.0000e+00, -0.0000e+00, -2.2044e-01,  0.0000e+00,  0.0000e+00,\n          0.0000e+00],\n        [ 1.0869e+00,  0.0000e+00,  6.1012e-01, -9.0003e-01, -0.0000e+00,\n          0.0000e+00,  5.5789e-01, -0.0000e+00,  8.4339e-01,  1.0397e-01,\n          0.0000e+00, -1.5533e+00, -0.0000e+00, -2.4743e-02,  4.0090e-01,\n         -0.0000e+00],\n        [ 0.0000e+00,  1.3076e-01,  0.0000e+00, -0.0000e+00, -0.0000e+00,\n          2.5988e-01,  0.0000e+00, -0.0000e+00,  2.1532e-01, -0.0000e+00,\n          4.6683e-01, -0.0000e+00, -0.0000e+00,  0.0000e+00,  1.2722e-01,\n          0.0000e+00]], grad_fn=<MulBackward0>)"
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gnn_model(data)\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T19:03:13.356261675Z",
     "start_time": "2024-02-08T19:03:13.314446196Z"
    }
   },
   "id": "1bc386961a93333c",
   "execution_count": 297
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for the GNN Model***!!!! \n",
      "conv_first.bias.grad None 0\n",
      "conv_first.lin.weight.grad None 0\n",
      "conv_out.bias.grad None 0\n",
      "conv_out.lin.weight.grad None 0\n",
      "Epoch 0 Loss tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
      "Gradients for the GNN Model!!!! \n",
      "conv_first.bias.grad tensor([ 0.0000, -0.1815,  0.0000,  0.2138,  0.0000, -0.0412, -0.0080, -0.0118,\n",
      "         0.0000,  0.0919, -0.0499,  0.0471,  0.0000,  0.0205,  0.1852,  0.0035]) 0\n",
      "conv_first.lin.weight.grad tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0255, -0.0535, -0.0121, -0.0261, -0.0335, -0.0372, -0.0382],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0281,  0.0609,  0.0135,  0.0298,  0.0385,  0.0415,  0.0426],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0040, -0.0045, -0.0021, -0.0056, -0.0061, -0.0054, -0.0031],\n",
      "        [-0.0008, -0.0013, -0.0004, -0.0008, -0.0010, -0.0010, -0.0009],\n",
      "        [-0.0010, -0.0014, -0.0005, -0.0015, -0.0016, -0.0015, -0.0009],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0110,  0.0239,  0.0058,  0.0125,  0.0159,  0.0175,  0.0168],\n",
      "        [-0.0046, -0.0058, -0.0027, -0.0038, -0.0047, -0.0073, -0.0049],\n",
      "        [ 0.0050,  0.0087,  0.0018,  0.0045,  0.0058,  0.0063,  0.0060],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0015,  0.0053,  0.0005,  0.0013,  0.0019,  0.0025,  0.0036],\n",
      "        [ 0.0237,  0.0516,  0.0115,  0.0250,  0.0322,  0.0359,  0.0356],\n",
      "        [ 0.0006,  0.0012,  0.0003,  0.0006,  0.0008,  0.0009,  0.0009]]) 0\n",
      "conv_out.bias.grad tensor([-0.3841]) 0\n",
      "conv_out.lin.weight.grad tensor([[ 0.0000e+00, -1.2624e-01,  0.0000e+00, -1.9110e-02,  0.0000e+00,\n",
      "         -7.1199e-03, -6.5318e-02, -7.4280e-03,  0.0000e+00, -7.3836e-02,\n",
      "         -1.4347e-03, -2.4655e-02,  0.0000e+00, -9.2062e-05, -8.8903e-02,\n",
      "         -1.0074e-02]]) 0\n",
      "Gradients for the GNN Model***!!!! \n",
      "conv_first.bias.grad None 1\n",
      "conv_first.lin.weight.grad None 1\n",
      "conv_out.bias.grad None 1\n",
      "conv_out.lin.weight.grad None 1\n",
      "Epoch 1 Loss tensor(0.2082, grad_fn=<MseLossBackward0>)\n",
      "Gradients for the GNN Model!!!! \n",
      "conv_first.bias.grad tensor([ 0.0000,  0.6293,  0.0000,  0.0000,  0.0000,  0.3077,  0.1090,  0.1778,\n",
      "         0.0000, -0.0543,  0.7069,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]) 1\n",
      "conv_first.lin.weight.grad tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0829,  0.1734,  0.0364,  0.0793,  0.1045,  0.1154,  0.1218],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0355,  0.0657,  0.0139,  0.0319,  0.0407,  0.0459,  0.0462],\n",
      "        [ 0.0115,  0.0191,  0.0047,  0.0102,  0.0130,  0.0143,  0.0136],\n",
      "        [ 0.0261,  0.0447,  0.0128,  0.0270,  0.0333,  0.0360,  0.0325],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0117, -0.0209, -0.0052, -0.0111, -0.0142, -0.0155, -0.0159],\n",
      "        [ 0.1037,  0.1885,  0.0497,  0.1067,  0.1350,  0.1429,  0.1370],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]) 1\n",
      "conv_out.bias.grad tensor([0.8081]) 1\n",
      "conv_out.lin.weight.grad tensor([[0.0000, 0.5797, 0.0000, 0.0000, 0.0000, 0.1677, 0.3074, 0.2026, 0.0000,\n",
      "         0.0092, 0.1958, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 30\u001B[0m\n\u001B[1;32m     27\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m---> 30\u001B[0m     gnn_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGradients for the GNN Model***!!!! \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m# Track gradients (gnn_model) and model parameters in TensorBoard\u001B[39;00m\n",
      "File \u001B[0;32m/snap/dataspell/75/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:755\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    753\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 755\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_wait_suspend(thread, frame, event, arg)\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    757\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m/snap/dataspell/75/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_args[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdo_wait_suspend(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/snap/dataspell/75/plugins/python-ce/helpers/pydev/pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[0;32m/snap/dataspell/75/plugins/python-ce/helpers/pydev/pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1199\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Testing GNNs Gradients Update\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Initialize GNNs\n",
    "input_dim = dataset.x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim = 16\n",
    "layer_num = 2\n",
    "dropout = 0.5\n",
    "lr = 0.08658681001095011\n",
    "weight_decay = 0.001358903786972319\n",
    "\n",
    "gnn_model = GCN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, layer_num=layer_num, dropout=dropout, hidden_dim_name=dataset_name)\n",
    "gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "gnn_criterion = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss() # \n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gnn_optimizer.zero_grad()\n",
    "    print(\"Gradients for the GNN Model***!!!! \")\n",
    "    # Track gradients (gnn_model) and model parameters in TensorBoard\n",
    "    for name, param in gnn_model.named_parameters():\n",
    "        print(f'{name}.grad', param.grad, epoch)\n",
    "        writer.add_histogram(name, param, epoch)\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram(f'{name}.grad', param.grad, epoch)\n",
    "    \n",
    "    # Forward Pass through GNN Layers\n",
    "    gnn_model.train() # TODO: Do we Need this Training?\n",
    "    out = gnn_model(data).flatten()\n",
    "    gnn_loss = gnn_criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    # gnn_loss = torch.tensor(random.random(), requires_grad=True)\n",
    "    # print(\"random loss %s\" % gnn_loss)\n",
    "    gnn_loss.backward()\n",
    "    gnn_optimizer.step()\n",
    "    print(\"Epoch %d Loss %s\" % (epoch, gnn_loss))\n",
    "       \n",
    "\n",
    "    print(\"Gradients for the GNN Model!!!! \")\n",
    "    # Track gradients (gnn_model) and model parameters in TensorBoard\n",
    "    for name, param in gnn_model.named_parameters():\n",
    "        print(f'{name}.grad', param.grad, epoch)\n",
    "        writer.add_histogram(name, param, epoch)\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram(f'{name}.grad', param.grad, epoch)\n",
    "\n",
    "    writer.add_scalar('Loss/train', gnn_loss.item(), epoch)\n",
    "\n",
    "print(\"Training finished.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T02:42:04.396627997Z",
     "start_time": "2024-02-09T02:17:47.433307805Z"
    }
   },
   "id": "f800ebb6bd88d658",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Unused code: \n",
    "\n",
    "# writer.add_scalar('Loss/train', gnn_loss.item(), epoch)\n",
    "\n",
    "# Track gradients (downstream_model) and model parameters in TensorBoard\n",
    "# for name, param in downstream_model.named_parameters():\n",
    "#     writer.add_histogram(name, param, epoch)\n",
    "#     if param.grad is not None:\n",
    "#         writer.add_histogram(f'{name}.grad', param.grad, epoch)\n",
    "#\n",
    "# writer.add_scalar('Loss/train', prediction_loss.item(), epoch)\n",
    "#\n",
    "# # print(\"Loss Value for Dim %d: %f\" % (i, prediction_loss))\n",
    "# dim_losses.append(prediction_loss)\n",
    "# all_losses.append(prediction_loss)\n",
    "#\n",
    "# print(\"Best Dimension is %s and Loss %s\" % (dim_losses.index(min(dim_losses)), min(dim_losses)))\n",
    "# with torch.no_grad():\n",
    "#     downstream_model.eval()\n",
    "#     outputs = downstream_model(X_test_tensor)\n",
    "#     _, predicted = torch.max(outputs, 1)\n",
    "#     accuracy = torch.sum(predicted == y_test_tensor).item() / len(y_test)\n",
    "#     print(f'Test Accuracy: {accuracy}')\n",
    "#     accuracies.append(accuracy)\n",
    "#\n",
    "# X = X_sliced[dim_losses.index(min(dim_losses))]\n",
    "# # X.reset_index(drop=True, inplace=True)\n",
    "# # y.reset_index(drop=True, inplace=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "#\n",
    "# # Convert to PyTorch Tensors\n",
    "# X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "# y_train_tensor = torch.LongTensor(y_train.to_numpy())\n",
    "# X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "# y_test_tensor = torch.LongTensor(y_test.to_numpy())\n",
    "#\n",
    "# # Create DataLoader for Training\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#\n",
    "# for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "#     outputs = downstream_model(inputs)\n",
    "#     prediction_loss = prediction_criterion(outputs, labels)\n",
    "#     # print(\"Outputs %s\\nLabels: %s\" % (outputs, labels))\n",
    "#     gnn_loss = gnn_criterion(outputs, labels)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}, GNN Loss: {gnn_loss.item():.4f}, Phenotype Loss: {prediction_loss.item():.4f}\")\n",
    "#\n",
    "#\n",
    "#\n",
    "#     prediction_loss.backward(retain_graph=True)\n",
    "#     gnn_loss.backward(retain_graph=True)\n",
    "#\n",
    "#     # Gradient clipping\n",
    "#     torch.nn.utils.clip_grad_norm_(gnn_model.parameters(), 0.01)\n",
    "#     torch.nn.utils.clip_grad_norm_(downstream_model.parameters(), 0.01)\n",
    "#\n",
    "#     gnn_optimizer.step()\n",
    "#     prediction_optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1eaf1a307aaac2e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, GNN Loss: 0.0296,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 47\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Integrate embeddings in the original dataset\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# TODO: Refactor this Code and Consider a Better Way to Pick a Dimension, should we do backpropagate here to get better prediction loss\u001B[39;00m\n\u001B[1;32m     46\u001B[0m dim_losses \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 47\u001B[0m X_sliced \u001B[38;5;241m=\u001B[39m integrate_final_embeddings()\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dim_idx, original_dataset_with_embeddings \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(X_sliced):\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;66;03m# print(\"****** Dim %d\" % dim_idx)\u001B[39;00m\n\u001B[1;32m     52\u001B[0m     original_dataset_with_embeddings_new \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(original_dataset_with_embeddings)\n",
      "Cell \u001B[0;32mIn[7], line 27\u001B[0m, in \u001B[0;36mintegrate_final_embeddings\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m embeddings_dim \u001B[38;5;241m=\u001B[39m subgraph_nodes_embeddings\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# subgraph_nodes_embeddings = subgraph_nodes_embeddings.T\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Aggregating dimensions of the embedding space\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# print(\"explained variance ratio %s\" % explained_variance_ratio)\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# print(subgraph_nodes_embeddings[0])\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m subgraph_nodes_embeddings \u001B[38;5;241m=\u001B[39m embedding_autoencoder(torch\u001B[38;5;241m.\u001B[39mtensor(subgraph_nodes_embeddings\u001B[38;5;241m.\u001B[39mvalues, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[1;32m     28\u001B[0m subgraph_nodes_embeddings \u001B[38;5;241m=\u001B[39m subgraph_nodes_embeddings\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     30\u001B[0m subgraph_nodes_dict_inv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m((v, k) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m subgraph_nodes_dict\u001B[38;5;241m.\u001B[39mitems())\n",
      "Cell \u001B[0;32mIn[6], line 48\u001B[0m, in \u001B[0;36membedding_autoencoder\u001B[0;34m(nodes_embeddings)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m     47\u001B[0m     output \u001B[38;5;241m=\u001B[39m model(nodes_embeddings)\n\u001B[0;32m---> 48\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(output, nodes_embeddings)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n\u001B[1;32m     51\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/anaconda3/envs/GNNs/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/GNNs/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/GNNs/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mmse_loss(\u001B[38;5;28minput\u001B[39m, target, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction)\n",
      "File \u001B[0;32m/snap/dataspell/75/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:923\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    920\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m back \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    921\u001B[0m     \u001B[38;5;66;03m# if we're in a return, we want it to appear to the user in the previous frame!\u001B[39;00m\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_suspend(thread, step_cmd)\n\u001B[0;32m--> 923\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_wait_suspend(thread, back, event, arg)\n\u001B[1;32m    924\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    925\u001B[0m     \u001B[38;5;66;03m# in jython we may not have a back frame\u001B[39;00m\n\u001B[1;32m    926\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclear_run_state(info)\n",
      "File \u001B[0;32m/snap/dataspell/75/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_args[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdo_wait_suspend(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/snap/dataspell/75/plugins/python-ce/helpers/pydev/pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[0;32m/snap/dataspell/75/plugins/python-ce/helpers/pydev/pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1199\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Attempting to run with KNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "# Initialize GNNs\n",
    "input_dim = dataset.x.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim = 16\n",
    "layer_num = 2\n",
    "dropout = 0.5\n",
    "lr = 0.008658681001095011\n",
    "weight_decay = 0.001358903786972319\n",
    "\n",
    "gnn_model = GCN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, layer_num=layer_num, dropout=dropout, hidden_dim_name=dataset_name)\n",
    "# for Debugging; Try to Follow Each Loss Independently\n",
    "gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "gnn_criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "# wandb.init(project='debuggingnn')\n",
    "# wandb.watch(downstream_model, criterion, log='all')\n",
    "\n",
    "num_epochs = 10\n",
    "all_losses= []\n",
    "accuracies = []\n",
    "best_scores = []\n",
    "best_params = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gnn_optimizer.zero_grad()\n",
    "\n",
    "    # Forward Pass through GNN Layers\n",
    "    gnn_model.train() # TODO: Do we Need this Training?\n",
    "    out = gnn_model(data).flatten()\n",
    "    gnn_loss = gnn_criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, GNN Loss: {gnn_loss.item():.4f},\")\n",
    "\n",
    "    # Integrate embeddings in the original dataset\n",
    "    # TODO: Refactor this Code and Consider a Better Way to Pick a Dimension, should we do backpropagate here to get better prediction loss\n",
    "    dim_losses = []\n",
    "    X_sliced = integrate_final_embeddings()\n",
    "\n",
    "\n",
    "    for dim_idx, original_dataset_with_embeddings in enumerate(X_sliced):\n",
    "        # print(\"****** Dim %d\" % dim_idx)\n",
    "        original_dataset_with_embeddings_new = pd.DataFrame(original_dataset_with_embeddings)\n",
    "    \n",
    "        # original_dataset_with_embeddings_new.columns = original_dataset.columns\n",
    "        # pca = PCA()\n",
    "        # pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "        # Xt = pca.fit_transform(original_dataset_with_embeddings_new)\n",
    "        # PC1 = Xt[:,0]\n",
    "        # \n",
    "        # explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        # PC1_df = pd.DataFrame(PC1)\n",
    "        # \n",
    "        # PC1_correlation_with_phenotype = PC1_df.corrwith(dataset_associated_phenotype['FEV1pp_utah']).tolist()[0]\n",
    "        # print(\"PC1 Correlation with the Phenotype: %s\" % PC1_correlation_with_phenotype)\n",
    "        # print(\"Percentage of Variance Explained by PC1: %s\" % explained_variance_ratio[0])\n",
    "    \n",
    "        scaler = MinMaxScaler()\n",
    "        original_dataset_with_embeddings_new_scaled = scaler.fit_transform(original_dataset_with_embeddings_new)\n",
    "    \n",
    "        # Prediction using KNN\n",
    "        X_train, X_test, y_train, y_test = train_test_split(original_dataset_with_embeddings_new_scaled, complete_original_dataset['finalgold_visit'], test_size=0.3, random_state=0)\n",
    "    \n",
    "        grid_params = {'n_neighbors': [5, 7, 9, 11, 13, 15, 19, 21],\n",
    "                       'weights': ['uniform', 'distance'],\n",
    "                       'metric': ['minkowski', 'euclidean', 'manhattan'],\n",
    "                       'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "                       }\n",
    "        gs = GridSearchCV(KNeighborsClassifier(), grid_params, cv=3, n_jobs = -1)\n",
    "        g_res = gs.fit(X_train, y_train)\n",
    "        print(\"Best Score %s\" % g_res.best_score_)\n",
    "        print(\"Best Params %s\" % g_res.best_params_)\n",
    "        best_scores.append(g_res.best_score_)\n",
    "        best_params.append(g_res.best_params_)\n",
    "\n",
    "        # Create a KNN classifier Best Params {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
    "        knn = KNeighborsClassifier(n_neighbors=g_res.best_params_['n_neighbors'], algorithm='auto', metric=g_res.best_params_['metric'], weights=g_res.best_params_['weights'])\n",
    "        # Train the classifier\n",
    "        knn.fit(X_train, y_train)\n",
    "        # Make predictions on the test set\n",
    "        y_pred = knn.predict(X_test)\n",
    "        # Evaluate the accuracy of the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    \n",
    "\n",
    "    print(\"Best Accuracy %s\" % max(accuracies))\n",
    "    X = X_sliced[accuracies.index(max(accuracies))]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, complete_original_dataset['finalgold_visit'], test_size=0.3, random_state=0)\n",
    "   \n",
    "\n",
    "    # Create a KNN classifier Best Params {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
    "    knn = KNeighborsClassifier(n_neighbors=g_res.best_params_['n_neighbors'], algorithm='auto', metric=g_res.best_params_['metric'], weights=g_res.best_params_['weights'])\n",
    "    # Train the classifier\n",
    "    knn.fit(X_train, y_train)\n",
    "    # Make predictions on the test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "    # Evaluate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)    \n",
    "\n",
    "\n",
    "    total_loss = gnn_loss + 0.5 * (1 - accuracy)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Total Loss: {total_loss:.4f},\")\n",
    "    total_loss.backward()\n",
    "    gnn_optimizer.step()\n",
    "      \n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T21:19:48.949183033Z",
     "start_time": "2024-02-08T21:13:18.397037972Z"
    }
   },
   "id": "e00591cc36fc9086",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.4889705882352941"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracies)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:21:12.374941390Z",
     "start_time": "2024-01-30T05:21:12.371182656Z"
    }
   },
   "id": "8c25fbfd744be4ba",
   "execution_count": 69
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
