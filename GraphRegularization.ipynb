{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T22:11:49.387737Z",
     "start_time": "2024-04-04T22:11:48.294847Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T05:39:15.209962Z",
     "start_time": "2024-03-15T05:39:15.204812Z"
    }
   },
   "id": "b850725ed218ebbf",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000, -0.0232,  ..., -0.0158,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0232,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000, -0.0158,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]])\n",
      "Epoch [10/300], Task Loss: 1.1789, Graph Regularization Loss: 1.3737\n",
      "Epoch [20/300], Task Loss: 1.1520, Graph Regularization Loss: 1.3721\n",
      "Epoch [30/300], Task Loss: 1.1282, Graph Regularization Loss: 1.3707\n",
      "Epoch [40/300], Task Loss: 1.1068, Graph Regularization Loss: 1.3696\n",
      "Epoch [50/300], Task Loss: 1.0873, Graph Regularization Loss: 1.3687\n",
      "Epoch [60/300], Task Loss: 1.0694, Graph Regularization Loss: 1.3678\n",
      "Epoch [70/300], Task Loss: 1.0528, Graph Regularization Loss: 1.3670\n",
      "Epoch [80/300], Task Loss: 1.0372, Graph Regularization Loss: 1.3664\n",
      "Epoch [90/300], Task Loss: 1.0225, Graph Regularization Loss: 1.3658\n",
      "Epoch [100/300], Task Loss: 1.0087, Graph Regularization Loss: 1.3653\n",
      "Epoch [110/300], Task Loss: 0.9955, Graph Regularization Loss: 1.3650\n",
      "Epoch [120/300], Task Loss: 0.9830, Graph Regularization Loss: 1.3647\n",
      "Epoch [130/300], Task Loss: 0.9710, Graph Regularization Loss: 1.3646\n",
      "Epoch [140/300], Task Loss: 0.9596, Graph Regularization Loss: 1.3646\n",
      "Epoch [150/300], Task Loss: 0.9486, Graph Regularization Loss: 1.3647\n",
      "Epoch [160/300], Task Loss: 0.9381, Graph Regularization Loss: 1.3648\n",
      "Epoch [170/300], Task Loss: 0.9279, Graph Regularization Loss: 1.3651\n",
      "Epoch [180/300], Task Loss: 0.9182, Graph Regularization Loss: 1.3654\n",
      "Epoch [190/300], Task Loss: 0.9088, Graph Regularization Loss: 1.3660\n",
      "Epoch [200/300], Task Loss: 0.8997, Graph Regularization Loss: 1.3666\n",
      "Epoch [210/300], Task Loss: 0.8909, Graph Regularization Loss: 1.3674\n",
      "Epoch [220/300], Task Loss: 0.8824, Graph Regularization Loss: 1.3683\n",
      "Epoch [230/300], Task Loss: 0.8742, Graph Regularization Loss: 1.3692\n",
      "Epoch [240/300], Task Loss: 0.8662, Graph Regularization Loss: 1.3702\n",
      "Epoch [250/300], Task Loss: 0.8584, Graph Regularization Loss: 1.3712\n",
      "Epoch [260/300], Task Loss: 0.8508, Graph Regularization Loss: 1.3722\n",
      "Epoch [270/300], Task Loss: 0.8435, Graph Regularization Loss: 1.3733\n",
      "Epoch [280/300], Task Loss: 0.8363, Graph Regularization Loss: 1.3745\n",
      "Epoch [290/300], Task Loss: 0.8294, Graph Regularization Loss: 1.3756\n",
      "Epoch [300/300], Task Loss: 0.8226, Graph Regularization Loss: 1.3768\n",
      "Top 10 Task Weights: [[ 283   17  951 ...  751  256  223]\n",
      " [ 713  751 1110 ...  165 1179 1129]\n",
      " [ 101  346   36 ...  991  676  185]]\n",
      "accuracy: 0.5079\n",
      "Mean Squared Error: 0.7897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Define the linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "        # y_pred = torch.sigmoid(self.linear(x))\n",
    "        # return y_pred\n",
    "    \n",
    "# Define the graph Laplacian regularization loss\n",
    "class GraphRegularizationLoss(nn.Module):\n",
    "    def __init__(self, adjacency_matrix, lambda_reg):\n",
    "        super(GraphRegularizationLoss, self).__init__()\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "        # self.laplacian_matrix = laplacian_matrix\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, weights_matrix):\n",
    "        # L = torch.sum(self.adjacency_matrix * torch.norm(node_embeddings.unsqueeze(1) - node_embeddings.unsqueeze(0), dim=2)**2)\n",
    "        # L = torch.sum(self.adjacency_matrix * torch.abs(weights_matrix.t()))\n",
    "        # return self.lambda_reg * L\n",
    "\n",
    "        # Multiple Classes\n",
    "        loss_reg = 0\n",
    "        for weight_vector in weights_matrix:\n",
    "            L = torch.sum(self.adjacency_matrix * torch.abs(weight_vector))\n",
    "            loss_reg += L\n",
    "\n",
    "        return self.lambda_reg * loss_reg\n",
    "        # loss_reg = 0\n",
    "        # for weight_vector in weights_matrix:\n",
    "        #     # L = torch.trace(torch.mm(torch.mm(weight_matrix.t(), self.laplacian_matrix), weight_matrix))\n",
    "        #     # L = torch.sum(self.laplacian_matrix * torch.abs(weight_matrix))\n",
    "        #     L = torch.sum(torch.abs(self.laplacian_matrix * weight_vector.view(-1, 1)))\n",
    "        #     # print(\"L %s\" % L)\n",
    "        #     loss_reg += L\n",
    "        # return self.lambda_reg * loss_reg\n",
    "\n",
    "# COPDGene SOMASCAN 1.3 Datset\n",
    "COPDGene_SOMASCAN13 = pd.read_csv('/home/shussein/NetCO/data/SOMASCAN13/COPDGene_SOMASCAN13_subjects.csv')\n",
    "X = COPDGene_SOMASCAN13.loc[:,COPDGene_SOMASCAN13.columns != 'finalgold_visit']\n",
    "y = COPDGene_SOMASCAN13['finalgold_visit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.to_numpy())\n",
    "X_test_scaled = scaler.fit_transform(X_test.to_numpy())\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "adjacency_matrix = torch.tensor(pd.read_csv('../data/PPI_Yong/ppi_graph_1183_mRNA_updated_root_2656sub.csv', delimiter='\\t').to_numpy(),\n",
    "                                dtype=torch.float32)\n",
    "degree_matrix = torch.sum(adjacency_matrix, dim=1)\n",
    "degree_matrix_sqrt_inv = torch.diag(torch.pow(degree_matrix, -0.5))\n",
    "laplacian_matrix = torch.eye(adjacency_matrix.size(0)) - degree_matrix_sqrt_inv.matmul(adjacency_matrix).matmul(degree_matrix_sqrt_inv)\n",
    "print(laplacian_matrix)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train_tensor.shape[1]\n",
    "output_size = 3\n",
    "lambda_reg = 0.001\n",
    "learning_rate = 0.001\n",
    "num_epochs = 300\n",
    "\n",
    "# Instantiate the model and loss function\n",
    "model = LinearRegression(input_size, output_size)\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "graph_reg_loss = GraphRegularizationLoss(adjacency_matrix, lambda_reg)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)#.flatten()\n",
    "    loss_task = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # Compute graph regularization loss\n",
    "    weights = model.linear.weight\n",
    "    loss_reg = graph_reg_loss(model.linear.weight)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = loss_task # + loss_reg\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Task Loss: {loss_task.item():.4f}, Graph Regularization Loss: {loss_reg.item():.4f}')\n",
    "\n",
    "# Print the Indicies of the Top 10 Contributing Features\n",
    "\n",
    "top_indices = np.argsort(-np.array(weights.detach().numpy()))[:10]\n",
    "print(\"Top 10 Task Weights: %s\" % top_indices)\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)#.detach().numpy()\n",
    "    # print(outputs)\n",
    "    # print(\"Predicted %s\" % outputs)\n",
    "   \n",
    "    # predicted_probs = torch.softmax(outputs, dim=1)\n",
    "    _, predicted_labels = torch.max(outputs, 1)\n",
    "    # print(predicted_labels)\n",
    "    accuracy = torch.sum(predicted_labels == y_test_tensor).item() / len(y_test)\n",
    "    mse = nn.MSELoss()(predicted_labels, y_test_tensor.float())\n",
    "    print(\"accuracy: {:.4f}\".format(accuracy))\n",
    "    print(f'Mean Squared Error: {mse:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T22:14:05.292703Z",
     "start_time": "2024-04-04T22:14:03.817519Z"
    }
   },
   "id": "f7795fba8c9ab7e9",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tensor([[-3.3943e-02, -3.7290e-02, -3.3233e-02],\n",
    "        [-7.1744e-02, -1.4546e-02, -5.1411e-02],\n",
    "        [-2.4098e-02, -5.7717e-03,  3.4128e-02],\n",
    "        [-3.4024e-02,  1.8222e-02,  1.3344e-03],\n",
    "        [-1.2702e-02, -1.1512e-01,  4.0245e-02],\n",
    "        [-5.3551e-02,  1.2136e-01,  7.5814e-02],\n",
    "        [-2.6230e-02, -6.1743e-02, -2.1658e-02],\n",
    "        [-3.8514e-02,  1.5583e-02, -7.0085e-02],\n",
    "        [ 4.0977e-02,  9.9375e-02,  1.2206e-02],\n",
    "        [ 1.9775e-02, -1.9514e-01, -1.1424e-01],\n",
    "        [ 4.1035e-02, -1.2258e-02,  2.3415e-02],"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T04:22:00.148299Z",
     "start_time": "2024-03-15T04:22:00.146879Z"
    }
   },
   "id": "433b7afc76a4e2fb",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2a37dbda1ca7b9f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ab876bbae1fef9aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
