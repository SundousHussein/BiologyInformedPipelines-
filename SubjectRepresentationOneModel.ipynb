{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pyreadstat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.transforms import RandomNodeSplit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:56:50.737922619Z",
     "start_time": "2024-02-13T17:56:50.734096074Z"
    }
   },
   "id": "9cd2c7b36e613757",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3575ee4ecb5ec337"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ad5cc791b7fb9ba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    # Preparing the Graph Dataset\n",
    "    DATASET_DIR = '/home/shussein/NetCO/GNNs/data/COPD/SparsifiedNetworks'\n",
    "    dataset_name = 'trimmed_fev1_0.515_0.111_adj.csv'\n",
    "\n",
    "    graph_adj_file = os.path.join(DATASET_DIR, dataset_name)\n",
    "    graph_adj = pd.read_csv(graph_adj_file, index_col=0).to_numpy()\n",
    "    nodes_names = pd.read_csv(graph_adj_file, index_col=0).index.tolist()\n",
    "\n",
    "    original_dataset_sid = pd.read_csv('/home/shussein/NetCO/GNNs/data/COPD/SparsifiedNetworks/fev1_X.csv', index_col=0)\n",
    "    original_dataset_sid.index.name = 'sid'\n",
    "\n",
    "    clinical_variables, meta = pyreadstat.read_sas7bdat(\"/home/shussein/NetCO/Data/COPDGene_P1P2P3_SM_NS_Long_Oct22.sas7bdat\")\n",
    "    clinical_variables = clinical_variables.set_index('sid')\n",
    "    # Filtering on Visit Number\n",
    "    clinical_variables = clinical_variables[clinical_variables['visitnum'] == 2.0]\n",
    "\n",
    "    clinical_variables_comorbidities = ['Angina', 'CongestHeartFail', 'CoronaryArtery', 'HeartAttack', 'PeriphVascular',\n",
    "                                        'Stroke', 'TIA', 'Diabetes',\n",
    "                                        'Osteoporosis', 'HighBloodPres', 'HighCholest', 'CognitiveDisorder',\n",
    "                                        'MacularDegen', 'KidneyDisease', 'LiverDisease']\n",
    "    clinical_variables = clinical_variables.assign(comorbidities=clinical_variables[clinical_variables_comorbidities].sum(axis=1))\n",
    "\n",
    "    complete_original_dataset = pd.merge(clinical_variables, original_dataset_sid, left_index=True, right_index=True)\n",
    "    clinical_variables = complete_original_dataset[clinical_variables.columns]\n",
    "\n",
    "    complete_original_dataset['finalgold_visit'].fillna(0, inplace=True)\n",
    "\n",
    "    complete_original_dataset.drop(complete_original_dataset[complete_original_dataset['finalgold_visit'] == -1].index, inplace=True)\n",
    "    original_dataset_sid = original_dataset_sid[original_dataset_sid.index.isin(complete_original_dataset.index.tolist())]\n",
    "    clinical_variables_cols = ['gender', 'age_visit', 'Chronic_Bronchitis', 'PRM_pct_emphysema_Thirona',\n",
    "                               'PRM_pct_normal_Thirona', 'Pi10_Thirona', 'comorbidities']\n",
    "\n",
    "    # Unifying Classes 0, -1, {1, 2} -> 1, {3, 4} -> 2\n",
    "    complete_original_dataset['finalgold_visit'] = np.where(complete_original_dataset['finalgold_visit'] == 2, 1,\n",
    "                                                            complete_original_dataset['finalgold_visit'])\n",
    "    complete_original_dataset['finalgold_visit'] = np.where((complete_original_dataset['finalgold_visit'] == 3) |\n",
    "                                                            (complete_original_dataset['finalgold_visit'] == 4), 2,\n",
    "                                                            complete_original_dataset['finalgold_visit'])\n",
    "\n",
    "    original_dataset_sid['finalgold_visit'] = complete_original_dataset['finalgold_visit']\n",
    "\n",
    "    graph = nx.from_numpy_array(graph_adj)\n",
    "\n",
    "    nodes_features = []\n",
    "    for node_name in nodes_names:\n",
    "        node_features = []\n",
    "        for clinical_variable in clinical_variables_cols:\n",
    "            node_features.append(\n",
    "                abs(original_dataset_sid[node_name].corr(\n",
    "                    complete_original_dataset[clinical_variable].astype('float64'))))\n",
    "        nodes_features.append(node_features)\n",
    "\n",
    "    features = np.array(nodes_features)\n",
    "    graph.remove_edges_from(nx.selfloop_edges(graph))\n",
    "\n",
    "    x = np.zeros(features.shape)\n",
    "    graph_nodes = list(graph.nodes)\n",
    "    for m in range(features.shape[0]):\n",
    "        x[graph_nodes[m]] = features[m]\n",
    "    x = torch.from_numpy(x).float()\n",
    "\n",
    "    # Edges Indexes\n",
    "    edge_index = np.array(list(graph.edges))\n",
    "    edge_index = np.concatenate((edge_index, edge_index[:, ::-1]), axis=0)\n",
    "    edge_index = torch.from_numpy(edge_index).long().permute(1, 0)\n",
    "\n",
    "    # Edges Weights\n",
    "    edge_weight = np.array(list(nx.get_edge_attributes(graph, 'weight').values()))\n",
    "    edge_weight = np.concatenate((edge_weight, edge_weight), axis=0)\n",
    "    edge_weight = torch.from_numpy(edge_weight).float()\n",
    "\n",
    "    dataset = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=torch.zeros(len(nodes_names)))\n",
    "\n",
    "    transform = RandomNodeSplit(num_val=5, num_test=5)\n",
    "    data = transform(dataset)\n",
    "    return data, original_dataset_sid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:56:55.176955397Z",
     "start_time": "2024-02-13T17:56:55.170509913Z"
    }
   },
   "id": "f2acbf0e9ee038c1",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-14T04:00:13.569562301Z",
     "start_time": "2024-02-14T04:00:12.181373651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimenions of Embeddings after GCN\n",
      "torch.Size([27, 128])\n",
      "tensor([[0.0654, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0782, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0000],\n",
      "        [0.0372, 0.0000, 0.0000,  ..., 0.0000, 0.0353, 0.0000],\n",
      "        ...,\n",
      "        [0.0880, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.2048, 0.0000, 0.0000],\n",
      "        [0.1143, 0.0000, 0.0000,  ..., 0.0570, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Dimensions of Embeddings after Autoencoder\n",
      "torch.Size([27, 128])\n",
      "tensor([[ 0.0784,  0.0464,  0.3894,  ..., -0.5173, -0.1933, -0.2545],\n",
      "        [ 0.0786,  0.0465,  0.3891,  ..., -0.5171, -0.1935, -0.2545],\n",
      "        [ 0.0786,  0.0465,  0.3892,  ..., -0.5172, -0.1935, -0.2545],\n",
      "        ...,\n",
      "        [ 0.0786,  0.0465,  0.3891,  ..., -0.5171, -0.1935, -0.2545],\n",
      "        [ 0.0793,  0.0469,  0.3882,  ..., -0.5164, -0.1941, -0.2545],\n",
      "        [ 0.0784,  0.0464,  0.3894,  ..., -0.5173, -0.1933, -0.2545]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 144\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs_num):\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m--> 144\u001B[0m     output \u001B[38;5;241m=\u001B[39m model(graph_data, X_train_scaled)\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;66;03m# Compute loss\u001B[39;00m\n\u001B[1;32m    146\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(output, y_train_tensor)\n",
      "File \u001B[0;32m~/anaconda3/envs/GNNs/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/GNNs/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[17], line 97\u001B[0m, in \u001B[0;36mNeuralNetwork.forward\u001B[0;34m(self, graph_data, original_dataset)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28mprint\u001B[39m(nodes_embeddings_ae)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# Multiplying the original_dataset with the generated embeddings\u001B[39;00m\n\u001B[0;32m---> 97\u001B[0m original_dataset_with_embeddings \u001B[38;5;241m=\u001B[39m original_dataset \u001B[38;5;241m*\u001B[39m nodes_embeddings_ae\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimensions of the Dataset after Integration\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28mprint\u001B[39m(original_dataset_with_embeddings\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_num=2, dropout=True, **kwargs):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.dropout = dropout\n",
    "        self.conv_first = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv_hidden = nn.ModuleList([GCNConv(hidden_dim, hidden_dim) for i in range(layer_num - 2)])\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.conv_first(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if self.dropout:\n",
    "            x = F.dropout(x, training=self.training)\n",
    "\n",
    "        for i in range(self.layer_num-2):\n",
    "            x = self.conv_hidden[i](x.clone(), edge_index, edge_weight)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "            if self.dropout:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module): # TODO: Need to revise this architecture\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 8),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),         \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, encoding_dim) \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 8),           \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, input_dim)    \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class DownstreamTaskNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(DownstreamTaskNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 gcn_input_dim, \n",
    "                 gcn_hidden_dim, \n",
    "                 gcn_layer_num, \n",
    "                 ae_encoding_dim, \n",
    "                 nn_input_dim, \n",
    "                 nn_hidden_dim1, \n",
    "                 nn_hidden_dim2, \n",
    "                 nn_output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.gcn = GCN(input_dim=gcn_input_dim, hidden_dim=gcn_hidden_dim, layer_num=gcn_layer_num)\n",
    "        self.autoencoder = Autoencoder(input_dim=gcn_hidden_dim, encoding_dim=ae_encoding_dim)\n",
    "        self.predictor = DownstreamTaskNN(nn_input_dim, nn_hidden_dim1, nn_hidden_dim2, nn_output_dim)\n",
    "\n",
    "    def forward(self, graph_data, original_dataset):\n",
    "        nodes_embeddings = self.gcn(graph_data) \n",
    "        print(\"Dimenions of Embeddings after GCN\")\n",
    "        print(nodes_embeddings.shape)\n",
    "        print(nodes_embeddings)\n",
    "        nodes_embeddings_ae = self.autoencoder(nodes_embeddings)\n",
    "        print(\"Dimensions of Embeddings after Autoencoder\")\n",
    "        print(nodes_embeddings_ae.shape)\n",
    "        print(nodes_embeddings_ae)\n",
    "        \n",
    "        # Multiplying the original_dataset with the generated embeddings\n",
    "        original_dataset_with_embeddings = original_dataset * nodes_embeddings_ae\n",
    "        print(\"Dimensions of the Dataset after Integration\")\n",
    "        print(original_dataset_with_embeddings.shape)\n",
    "        print(original_dataset_with_embeddings)\n",
    "        predictions = self.predictor(original_dataset_with_embeddings)\n",
    "        print(\"Dimensions of the Dataset after Prediction\")\n",
    "        print(predictions.shape)\n",
    "        print(predictions)\n",
    "        return predictions\n",
    "\n",
    "graph_data, original_dataset = get_datasets()\n",
    "# TODO: Need to split the 'original_dataset' into training and testing \n",
    "X = original_dataset.drop(['finalgold_visit'], axis=1)\n",
    "y = original_dataset['finalgold_visit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train.to_numpy())\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.LongTensor(y_test.to_numpy())\n",
    "\n",
    "gcn_input_dim = graph_data.x.shape[1]\n",
    "gcn_hidden_dim = 128\n",
    "gcn_layer_num = 2\n",
    "ae_encoding_dim = 1\n",
    "nn_input_dim = X_train_tensor.shape[1]\n",
    "nn_hidden_dim1 = 64\n",
    "nn_hidden_dim2 = 16\n",
    "nn_output_dim = 3 # Number of Classes\n",
    "\n",
    "model = NeuralNetwork(gcn_input_dim=gcn_input_dim, gcn_hidden_dim=gcn_hidden_dim, gcn_layer_num=gcn_layer_num,\n",
    "                      ae_encoding_dim=ae_encoding_dim,\n",
    "                      nn_input_dim=nn_input_dim, nn_hidden_dim1=nn_hidden_dim1, nn_hidden_dim2=nn_hidden_dim2, nn_output_dim=nn_output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.030254, weight_decay=0.000100)\n",
    "\n",
    "epochs_num = 16\n",
    "# Training loop\n",
    "for epoch in range(epochs_num):\n",
    "    # Forward pass\n",
    "    output = model(graph_data, X_train_scaled)\n",
    "    # Compute loss\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss for monitoring\n",
    "    print(f\"Epoch [{epoch+1}/{epochs_num}], Loss: {loss.item()}\")\n",
    "    \n",
    "    print(\"*****************GRADIENTS********************\")\n",
    "    print(\"Gradients for the Model!!!! \")\n",
    "    # Track gradients (gnn_model) and model parameters in TensorBoard\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'{name}.grad', param.grad, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
